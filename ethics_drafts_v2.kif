(documentation Ethics EnglishLanguage "Ethics is the normative science of the conduct of human beings living in society, which judges this conduct to be right or wrong, to be good or bad, or in some similar way. (An Introduction to Ethics (LIllie, 1948))")

;; This might be questionable?
(instance Ethics FieldOfStudy)

;; Philosophy is technically an "instance" of FieldOfStudy... seems wrong.
;; (subclass Ethics Philosophy)
(subclass Ethics Science)

(documentation MoralAttribute EnglishLanguage "Moral Attributes are a subcass of Normative Attributes intended to denote whether something is Good, Bad, Right, Wrong, Virtuous, Viceful, or other moral attributes.")
(subclass MoralAttribute NormativeAttribute)

(instance MorallyGood MoralAttribute)
(instance MorallyBad MoralAttribute)
;; MorallyBetter, MorallyWorse, 

(subclass VirtueAttribute MoralAttribute)
(subclass ViceAttribute MoralAttribute)

(subclass VirtueAttribute PsychologicalAttribute)
(subclass ViceAttribute PsychologicalAttribute)

;; Generally speaking, yes.  Might some paraconsistency reign?  :- p
(contraryAttribute MorallyGood MorallyBad)
(contraryAttribute VirtueAttribute ViceAtribute)

;; I see how this definition motivates the need for target-specificity or domain-specificity
(documentation VirtuousAgent EnglishLanguage "'A virtuous agent is one who has, and exercises, certain character traits, namely, the virtues.' (On Virtue Ethics)")
(subclass VirtuousAgent AutonomousAgent)
(=>
    (and
        (instance ?AGENT AutonomousAgent)
        (instance ?VIRTUE VirtueAttribute)
        (attribute ?AGENT ?VIRTUE))
    (instance ?AGENT VirtuousAgent))

(documentation ViciousAgent EnglishLanguage "A vicious agent is one who has, and exercises, certain character traits, namely, the vices.  The antonym of VirtuousAgent.")
(subclass ViciousAgent AutonomousAgent)
(=>
    (and
        (instance ?AGENT AutonomousAgent)
        (instance ?VICE ViceAttribute)
        (attribute ?AGENT ?VICE))
    (instance ?AGENT VirtuousAgent))

;; Encapsulate the Autonomous Agent aspect of "behavior"-type processes.
(documentation AutonomousAgentProcess EnglishLanguage "AgentProcess is the Class of all Processes in which there is an autonomous agent.")
(subclass AutonomousAgentProcess Process)
(subclass BodyMotion AutonomousAgentProcess)
(subclass Vocalizing AutonomousAgentProcess)

(=>
    (instance ?PROC AutonomousAgentProcess)
    (exists (?AGENT)
        (and 
            (agent ?PROC ?AGENT)
            (instance ?AGENT AutonomousAgent))))


(documentation MoralJudging EnglishLanguage "A subclass of Judging where the proposition believed is the assignment of a moral attribute to a behavior.")
(subclass MoralJudging Judging)

;; Decided that the behavior can be a patient of the MoralJudging
;; And the moral judgment can be the result.
;; There is a precedent in Adam's SUMO.
(=>
    (instance ?JUDGE MoralJudging)
    (exists (?BEHAVE ?MORAL)
        (and 
            (instance ?BEHAVE AutonomousAgentProcess)
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE ?BEHAVE)
            (result ?JUDGE 
                (modalAttribute ?BEHAVE ?MORAL)))))

;; Ethics refers to the moral judging of processes (behavior) of members of groups (aka society, lol).
;; Not sure how to get the "normative science" part in.  I think there's a lot of hidden baggage in the term "normative".
(and 
    (refers Ethics ?JUDGE)
    (instance ?JUDGE MoralJudging)
    (instance ?MORAL MoralAttribute)
    (instance ?GROUP Group)
    (instance ?BEHAVE AutonomousAgentProcess)
    (member ?MEMB ?GROUP)
    (agent ?BEHAVE ?MEMB)
    (patient ?JUDGE ?BEHAVE)
    (result ?JUDGE 
        (modalAttribute ?BEHAVE ?MORAL)))

(subclass MoralNihilism Ethics)
(subclass Deontology Ethics)
(subclass Utilitarianism Ethics)
(subclass VirtueEthics Ethics)

;; Take one on Moral Nihilism: Nothing has any moral attributes.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Stating)
    (patient ?STATE 
        (not 
            (exists (?BEHAVE ?MORAL) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (instance ?MORAL MoralAttribute)
                    (modalAttribute ?BEHAVE ?MORAL))))))

;; Take two: "Nothing is morally wrong." (from SEP Moral Skepticism); 
;;technically, this would be a noncognitivist nihilism, I suppose (citing Ethics: The Fundamentals)
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Stating)
    (patient ?STATE 
        (not 
            (exists (?BEHAVE) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; Take three without the "Stating" -- My favorite.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?BEHAVE) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; Take four without the "Statement"
(refers MoralNihilism 
    (not 
        (exists (?BEHAVE) 
            (and 
                (instance ?BEHAVE AutonomousAgentProcess)
                (modalAttribute ?BEHAVE MorallyBad)))))                  

;; "Moral nihilism is the view that there are no moral facts." (Ethics: The FUudamental)
;; There is no moral judging (with behavior and moral judgments) that is a fact.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?JUDGE) 
                (and 
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE ?MORALSTATEMENT)
                    (instance ?MORALSTATEMENT Fact))))))

;; Deontology refers to statements of the nature that there exist rules such that 
;; if followed, behavior is good or if not followed, behavior is bad.
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (=>
                    (conforms ?BEHAVE ?RULE)
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyGood)))))
            (exists (?RULE)
                (=>
                    (not 
                        (conforms ?BEHAVE ?RULE))
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyBad))))))))       
                            
(documentation conformsFormula EnglishLanguage "(conforms ?OBJ ?FORMULA) describes how ?OBJ follows the ideas outlined by the proposition represented by ?FORMULA.")

(domain conformsFormula 1 Object)
(domain conformsFormula 2 Formula)
(instance conformsFormula BinaryPredicate)
(subrelation conformsFormula represents)

;; Take one
;; Q: should I use realization or conforms?
(=> 
    (conformsFormula ?OBJ ?FORMULA)
    (and 
        (containsInformation ?FORMULA ?PROP)
        (conforms ?OBJ ?PROP)))

;; Take two -- should I say that this proposition the formula is expressing exists?
(=> 
    (conformsFormula ?OBJ ?FORMULA)
    (exists (?PROP)
        (and 
            (containsInformation ?FORMULA ?PROP)
            (conforms ?OBJ ?PROP))))

;; Take two: specify that rules are formulas (of suo-kif :-p)       
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (=>
                    (conformsFormula ?BEHAVE ?RULE)
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyGood)))))
            (exists (?RULE)
                (=>
                    (not 
                        (conformsFormula ?BEHAVE ?RULE))
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyBad))))))))

;; Take three: specify that the rules correspond to Deontic Attributes
;; If a rule adheres to obligation, good; if a rule doesn't, bad.
;; If a rule violates a prohibition, good; otherwise ... unspecified?
;; If an action prevents an action that is permitted, bad.
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Obligation)
                    (=>
                        (conformsFormula ?BEHAVE ?RULE)
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE
                                (modalAttribute ?BEHAVE MorallyGood))))
                    (=>
                        (not 
                            (conformsFormula ?BEHAVE ?RULE))
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE
                                (modalAttribute ?BEHAVE MorallyBad))))))
             (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Prohibition)
                    (=> 
                        (conformsFormula ?BEHAVE ?RULE)
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE
                                (modalAttribute ?BEHAVE MorallyBad))))))
            (exists (?RULE) 
                (and 
                    (modalAttribute ?RULE Permission)
                    (=> 
                        (and 
                            (conformsFormula ?BEHAVE1 ?RULE)
                            (prevents ?BEHAVE2 ?BEHAVE1)
                            (instance ?BEHAVE2 AutonomousAgentProcess))
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE 
                                (modalAttribute ?BEHAVE2 MorallyBad)))))))))  


;; Take four: remove the MoralJudging part as maybe the statement should be that it just "is" MorallyBad :D.
;; Although this sort of bypasses the definition of ethics as concerned with moral judgments. 
;; However, it may still fit the definition of a moral judgment!
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Obligation)
                    (=>
                        (conformsFormula ?BEHAVE ?RULE)
                        (modalAttribute ?BEHAVE MorallyGood))
                    (=>
                        (not 
                            (conformsFormula ?BEHAVE ?RULE))
                        (modalAttribute ?BEHAVE MorallyBad))))
             (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Prohibition)
                    (=> 
                        (conformsFormula ?BEHAVE ?RULE)
                        (modalAttribute ?BEHAVE MorallyBad))))
            (exists (?RULE) 
                (and 
                    (modalAttribute ?RULE Permission)
                    (=> 
                        (and 
                            (conformsFormula ?BEHAVE1 ?RULE)
                            (prevents ?BEHAVE2 ?BEHAVE1)
                            (instance ?BEHAVE2 AutonomousAgentProcess))
                        (modalAttribute ?BEHAVE2 MorallyBad)))))))              
               

;; Begin Virtue Ethics section

;; A set of the possible options in a decision... seems not needed :D
(subclass DecisionSet NonNullSet)

;; To be honest, while I think it's good to have this definition, I'm not sure it'll actually be needed :D.
(documentation DecisionOptionFn EnglishLanguage "A UnaryFunction that maps an instance of Deciding to the set of possibilities that are available.")
(domain DecisionOptionFn 1 Deciding)
(instance DecisionOptionFn TotalValuedRelation)
(instance DecisionOptionFn UnaryFunction)
(range DecisionOptionFn Set)

(=>
    (member ?P (DecisionOptionFn ?DECIDE))
    (patient ?DECIDE ?P))

(=> 
    (patient ?DECIDE ?P)
    (member ?P (DecisionOptionFn ?DECIDE)))

;; I wish to use 'result' to denote the processes that is decided upon and 'patient' to denote those being chosen among.
;; So the result should be one of the processes being considered.
;; Not needed because 'result' is a subrelation of 'patient.
(=> 
    (and
        (instance ?DECIDE Deciding)
        (result ?DECIDE ?P))
    (patient ?DECIDE ?P))

;; "An action is right iff it is what a virtuous agent would characteristically (i.e. acting in character) do in the circumstances." (On Virtue Ethics -- Right Action)
;; Take one without the "would characteristically do" part.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (<=>
            (modalAttribute ?BEHAVE MorallyGood)
            (and 
                (agent ?BEHAVE ?AGENT)
                (instance ?AGENT VirtuousAgent)))))

;; Take two: perhaps one can phrame the "would do" in terms of the instance of moral judging.
;; An agent A will judge a behavior to be morally good if and only if
;; The agent A believes there exists a decision of a virtuous agent B resulting in the behavior.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ 
                (exists (?DECIDE)
                    (and 
                        (agent ?BEHAVE ?AGENTB)
                        (agent ?DECIDE ?AGENTB)
                        (instance ?AGENTB VirtuousAgent)
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)))))))
        
;; Take three: the above doesn't quite capture the conditional nature.
;; ... if the judging agent believes that in an identical decision landscape, 
;; the same choice would likely be made. 
;; "equals" might be too strong and force the agents to be the same.  "Similar" please?  
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)
                        (and 
                            (agent ?DECIDEV ?AGENTV)
                            (instance ?AGENTV VirtuousAgent)
                            (instance ?DECIDEV Deciding)
                            (equals (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))))
                    (modalAttribute (result ?DECIDEV ?BEHAVE) Likely))))))

;; Take four: given the weakness of 'equals', it seems similar to:
;; If the judging agent believes that if there is an instance of deciding by a virtuous agent
;; and the behavior is one of the options being decided upon,
;; then the behavior is likely to be chosen.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (agent ?DECIDE ?AGENTA)
                        (instance ?AGENTA VirtuousAgent)
                        (patient ?DECIDE ?BEHAVE))
                    (modalAttribute (result ?DECIDE BEHAVE) Likely))))))

;; The set of attributes applying to a Process.
(documentation ProcessAttributeFn EnglishLanguage "A UnaryFunction that maps a Process to the set of attributes that apply to it.")
(domain ProcessAttributeFn 1 Process)
(instance ProcessAttributeFn TotalValuedRelation)
(instance ProcessAttributeFn UnaryFunction)
(range ProcessAttributeFn Set)

(=>
    (attribute ?P ?ATT)
    (member ?ATT (ProcessAttributeFn ?P)))

(=> 
    (member ?ATT (ProcessAttributeFn ?P))
    (attribute ?P ?ATT))

;; Ok, I decided to implement an auxiliary function that returns all elements in Set1 that satisfy some binary predicate (aka similary measure) with regard to an element in the other set.
;; Basically, all elements in one set that are similar to an element in the other set.
(documentation ElementsShareBPFn EnglishLanguage "A TernaryFunction that maps a binary predicate (?BP) and two sets to the set of elements in the first set such that the binary predicate holds with an element in the second set.")
(domain ElementsShareBPFn 1 BinaryPredicate)
(domain ElementsShareBPFn 2 Set)
(domain ElementsShareBPFn 3 set)
(instance ElementsShareBPFn TernaryFunction)
(instance ElementsShareBPFn TotalValuedRelation)
(range ElementsShareBPFn Set)

(<=> 
    (member ?E1 (ElementsShareBPFn ?BP ?S1 ?S2))
    (and
        (member ?E1 ?S1)
        (exists (?E2) 
            (and 
                (member ?E2 ?S2)
                (?BP ?E1 ?E2)))))

;; This is nice yet with "sets of possible actions", we need to determine the "similarity" of the members and not their identity.
(documentation similarSets EnglishLanguage "A binary predicate that indicates whether two sets share most of their elements.")
(domain similarSets 1 Set)
(domain similarSets 2 Set)
(instance similarSets BinaryPredicate)

(<=> 
    (similarSets ?S1 ?S2)
    (or
            (and (instance ?S1 NullSet) (instance ?S2 Nullset))
            (and 
                (and (instance ?SP1 NonNullSet) (instance ?S2 NonNullset))
                (equal ?INT12 (IntersectionFn ?S1 ?S2))
                (equal ?UN12 (UnionFN ?S1 ?S2))
                (greaterThan (MultiplicationFn 2 (CardinalityFn ?INT12)) (CardinalityFn ?UP1)))))

;; So two sets here are similar if most of S1 are similar with some element of S2
;; And most of S2 are similar with some element of S1.
(documentation similarSetsWithBP EnglishLanguage "A ternary predicate that indicates whether most of each set satisfies a binary predicate with regard regard to an element in another set.")
(domain similarSetsWithBP 1 BinaryPredicate)
(domain similarSetsWithBP 2 Set)
(domain similarSetsWithBP 3 Set)
(instance similarSets TernaryPredicate)

(<=>
    (similarSetsWithBP ?BP ?S1 ?S2)
    (and 
        (greaterThan (MultiplicationFn 2 (CardinalityFn (ElementsShareBPFn ?BP ?S1 ?S2))) (CardinalityFn ?S1))
        (greaterThan (MultiplicationFn 2 (CardinalityFn (ElementsShareBPFn ?BP ?S2 ?S1))) (CardinalityFn ?S2))))

;; I think using this I can define similarity.
;; Ideally, I will also include CaseRoles and spatiotemporal relations.
(documentation similarProcesses EnglishLanguage "A binary predicate that indicates whether two process share most of their attributes.")
(domain similarProcesses 1 Process)
(domain similarProcesses 2 Process)
(instance similarProcesses BinaryPredicate)

(<=> 
    (similarProcesses ?P1 ?P2)
    (and 
        (equal ?SP1 (ProcessAttributeFn ?P1))
        (equal ?SP2 (ProcessAttributeFn ?P2))
        (similarSets ?SP1 ?SP2)))

;; Now I think "(similarSetsWithBP similarProcesses (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))"
;; Let's us say that the set of options available for the agent are similar to the set of options available for the virtuous agent.
;; For each possible action for the agent being judged,
;; Is there some possible action for the virtuous agent that is similar?
;; And vice-versa.

;; Take five: take three with equality replaced by similarity.
;; ... if the judging agent believes that in a similar decision landscape, 
;; the same choice would likely be made. 
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)
                        (and 
                            (agent ?DECIDEV ?AGENTV)
                            (instance ?AGENTV VirtuousAgent)
                            (instance ?DECIDEV Deciding)
                            (similarSetsWithBP similarProcesses (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))))
                    (modalAttribute (result ?DECIDEV ?BEHAVE) Likely))))))


;; The idea here is that if E1 and E2 are similar to agent A, 
;; Then A is likely to make similar judgments with regard to E1 and E2.
;; This should ontologically work for the Virtue ethics case above... without providing a specific measure.
(documentation Similarity EnglishLanguage "Similarity attempts to capture the ontologic notion of similarity from a subjective point of view.")
(instance Similarity TernaryPredicate) ;; or subclass?  Meh for now.  Just wanna sketch things out and move on.  
(domain Similarity 1 CognitiveAgent)
(domain Similarity 2 Entity)
(domain Similarity 3 Entity)

(=> 
    (Similarity ?A ?E1 ?E2)
    (=>
        (and
            (instance ?J1 Judging)
            (agent ?J1 ?A)
            (patient ?J1 ?E1)
            (result ?J1 ?O1)
            (instance ?J2 Judging)
            (agent ?J2 ?A)
            (patient ?J2 ?E2))
        (modalAttribute (result ?J2 ?O1) Likely)))

(<=> 
    (Similarity ?A ?E1 ?E2)
    (Similarity ?A ?E2 ?E1))

;; (and (result ?J2 ?O2) (Similarity ?A ?O1 ?O2))

;; Decided I should do the Vice version, extrapolating from the Virtuous behavior definition of right action.
;; The moral status of behavior that is neither virtuous nor vicious seems unclear.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)
                        (and 
                            (agent ?DECIDEV ?AGENTV)
                            (instance ?AGENTV ViciousAgent)
                            (instance ?DECIDEV Deciding)
                            (similarSetsWithBP similarProcesses (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))))
                    (modalAttribute (result ?DECIDEV ?BEHAVE) Likely))))))

;; Next up: Utilitarianism

;; Hey, GPT-4 suggested this :D
(documentation HedonisticUtilitarianism EnglishLanguage "Hedonistic Utilitarianism is a form of utilitarianism that focuses on maximizing pleasure and minimizing pain in evaluating the moral value of an action.")
(subclass HedonisticUtilitarianism Utilitarianism)

;; A utility function that maps an action to the net pleasure (pleasure - pain) it creates. (GPT-4 + Zar)
;; Does this actually 'exist'?  Well, that's up for debate.  It can be used in a high-level definition, however, like a black box function :- p.
;; In draft 1, I tried to sketch out how Utilitarianism as sketched out by John Stuart Mill works with aggregation functions,
;; So that the utility evaluations of each member of a group get aggregated into a single utility value for the group.
(documentation UtilityFn EnglishLanguage "A UnaryFunction that maps an instance of AutonomousAgentProcess to the net utility it creates.  In the case of hedonistic utilitarianism, this may be (pleasure - pain).")
(domain UtilityFn 1 AutonomousAgentProcess)
(instance UtilityFn TotalValuedRelation)
(instance UtilityFn UnaryFunction)
(range UtilityFn RealNumber)

;; Draft 1 (GPT-4 + Zar)
;; A morally good action is one which has a positive utility, i.e., increases pleasure more than it causes pain.
;; This is a bit naive yet highly simplistic.
;; If you have some magical utility function that applies to all behavior, then, yeah, you can just use it and determine if
;; the behavior is good or bad.  Best and worst require comparisons.
;; One could claim that this is the quintessence of Utilitarianism.  
;; The specific forms of consequentialism and utilitarianism are ways to further specify the nature of this magical utility function.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (and
                (instance ?BEHAVE AutonomousAgentProcess)
                (greaterThan (UtilityFn ?BEHAVE) 0)))))

;; A morally bad action according to Utilitarianism 
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad)))
            (and
                (instance ?BEHAVE AutonomousAgentProcess)
                (lessThan (UtilityFn ?BEHAVE) 0)))))

;; Draft 2 (GPT-4 + Zar)
;; This has the downside that only intentional behavior (resulting from decisions) can be judged as morally good or bad.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (forall (?OPTION)
                    (=> 
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThanOrEqualTo (UtilityFn ?BEHAVE) (UtilityFn ?OPTION))))))))

;; This is very brutal: if you have a better option and don't take it, your behavior is bad >:D.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad)))
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (exists (?OPTION)
                    (and
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThan (UtilityFn ?OPTION) (UtilityFn ?BEHAVE))))))))

;; Draft 3 (Zar, lol)
;; The "if and only if" above seems too strong.
;; If the option is the best available, then it is morally good by utilitarian standards.
;; We leave unspecified how to morally judge actions that don't result from an instance of Deciding (such as subconscious behavior).
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (instance ?BEHAVE AutonomousAgentProcess)
                (forall (?OPTION)
                    (=> 
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThanOrEqualTo (UtilityFn ?BEHAVE) (UtilityFn ?OPTION)))))
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood))))))

;; Likewise, if there's an instance of deciding resulting in a behavior and there exists a better option, 
;; then the behavior is morally bad by utilitarian standards.
 (and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (instance ?BEHAVE AutonomousAgentProcess)           
                (exists (?OPTION)
                    (and
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThan (UtilityFn ?OPTION) (UtilityFn ?BEHAVE)))))
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; Comments on GPT-4.  Unlike 3.5, it feels more like editing a rough draft than confused, botched crap that's not particularly usable (beyond random brainstorming).

;; Next up: perhaps define some UtilityFn in terms CausingPain and CausingHappiness?
;; Consequentialism? 
;; Or, perhaps, moving on to the next phase of the project before circling back to these definitions?
;; Ah, I could try to actually define some instances of these theories!
;; And get to the case examples.
;; Value systems?  

(documentation Consequentialism EnglishLanguage "Consequentialism is a moral theory that holds that 'whether an act is morally right depends only on consequences (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act)' (Stanford Encyclopedia of Philosophy).")
(subclass Consequentialism Utilitarianism)

;; Very general: An outcome is the physical entity that holds when a process ends.
;; (partition Physical Object Process).
(subclass Outcome Physical)

;; O is an outcome if and only if there exists some process P such that O is the result of P.
(<=>
    (instance ?OUTCOME Outcome)
    (exists (?P)
        (and 
            (instance ?P Process)
            (result ?P ?OUTCOME))))

;; If O is an outcome of a process ?P, then the end of ?$ is before the beginning of O.
;; In theory there could be some overlap, such as the pain caused by punching someone in the face...?
(=>
    (and 
        (instance ?OUTCOME Outcome)
        (result ?P ?OUTCOME))
    (before
        (EndFn (WhenFn ?P))
        (BeginFn (WhenFn ?OUTCOME))))

;; Draft 1
(and
    (refers Consequentialism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?JUDGE MoralJudging)
                (result ?JUDGE ?CONCLUSION)
                (equals ?CONCLUSION
                    (modalAttribute ?BEHAVE MorallyGood)))
            (exists (?ARGUMENT ?OUTCOME)
                (and 
                    (instance ?ARGUMENT Argument)
                    (instance ?OUTCOME Outcome)
                    (result ?BEHAVE ?OUTCOME)
                    (conclusion ?ARGUMENT ?CONCLUSION)
                    (premise ?ARGUMENT ?OUTCOME))))))

;; Decided to call the judgment the patient of the MoralJudging.  It's a subrelation of patient, so I don't think it requires any immediate updates to the above draft. 
;; Maybe I can say something like,
;; If there is an instance of moral judging,
;; Then there exists an argument, an outcome, and a premise such that the premise refers to the outcome and the conclusion is the moral assertion. 
;; The use of an Argument (deductive or inductive) seems to be overly stong as a requirement.
;; Moreover, for all premises of the argument that refer to something physical,
;; the physical entity must be an outcome of the behavior.  
;; Moreover, no premise refers to the attributes of the agent.
;; That is, the judgment process may depend on the ?OUTCOME in some fuzzy manner.
;; But for now, I'll go with this.  Draft 2:
(and
    (refers Consequentialism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?JUDGE MoralJudging)
                (result ?JUDGE ?CONCLUSION)
                (equals ?CONCLUSION
                    (modalAttribute ?BEHAVE MorallyGood)))
            (exists (?ARGUMENT ?OUTCOME ?PREMISE)
                (and 
                    (instance ?ARGUMENT Argument)
                    (instance ?OUTCOME Outcome)
                    (result ?BEHAVE ?OUTCOME)
                    (premise ?ARGUMENT ?PREMISE)
                    (refers ?PREMISE ?OUTCOME)
                    (conclusion ?ARGUMENT ?CONCLUSION)
                    (agent ?BEHAVE ?AGENT)
                    (forall (?PROP)
                        (and 
                            (=> 
                                (and
                                    (premise ?ARGUMENT ?PROP)
                                    (refers ?PROP ?PHYS)
                                    (instance ?PHYS Physical))
                                (and 
                                    (instance ?PHYS Outcome)
                                    (result ?BEHAVE ?PHYS)))
                            (=> 
                                (premise ?ARGUMENT ?PROP)
                                (forall (?ATT) 
                                    (=> 
                                        (attribute ?AGENT ?ATT))
                                        (not (refers ?PROP ?ATT)))))))))))

 ;; Note on GPT-4 -- in this case, it was not so easy to receive much help.
 ;; Probably because the challenge is how to actually philosophically frame the problem.
 ;; Utilitarianism based on a black-box UtilityFn is relatively simple (and commonplace).                   

;; Self-comment: I like using this huge file to "work in" while moving drafts to other files that are less messy.

;; Trolley Problem time.
;; First, fuck it, I will use Train.

(instance ?TROLLEY Train)
(instance ?TRACK1 Railway)
(instance ?TRACK2 Railway)
(instance ?TRACK3 Railway)
(instance ?FORK RailJunction)
(instance ?LEVER Lever)

;; The humans involved
(instance ?MORALAGENT Human)
(instance ?PERSON1 Human)
(instance ?PERSON2 Human)
(instance ?PERSON3 Human)
(instance ?PERSON4 Human)
(instance ?PERSON5 Human)
(instance ?PERSON6 Human)

;; The tedious inequalities (ty GPT-4 🙏):

(not (equal ?TRACK1 ?TRACK2))
(not (equal ?TRACK1 ?TRACK3))
(not (equal ?TRACK2 ?TRACK3))

(not (equal ?PERSON1 ?PERSON2))
(not (equal ?PERSON1 ?PERSON3))
(not (equal ?PERSON1 ?PERSON4))
(not (equal ?PERSON1 ?PERSON5))
(not (equal ?PERSON1 ?PERSON6))
(not (equal ?PERSON1 ?MORALAGENT))

(not (equal ?PERSON2 ?PERSON3))
(not (equal ?PERSON2 ?PERSON4))
(not (equal ?PERSON2 ?PERSON5))
(not (equal ?PERSON2 ?PERSON6))
(not (equal ?PERSON2 ?MORALAGENT))

(not (equal ?PERSON3 ?PERSON4))
(not (equal ?PERSON3 ?PERSON5))
(not (equal ?PERSON3 ?PERSON6))
(not (equal ?PERSON3 ?MORALAGENT))

(not (equal ?PERSON4 ?PERSON5))
(not (equal ?PERSON4 ?PERSON6))
(not (equal ?PERSON4 ?MORALAGENT))

(not (equal ?PERSON5 ?PERSON6))
(not (equal ?PERSON5 ?MORALAGENT))

(not (equal ?PERSON6 ?MORALAGENT))

;; Or we can use UniqueList, right?
;; Something which GPT-4 is also not bad at, with a bit of guidance and correcction.
;; One note is that for practical reasoning via Vampire, the tedious expansion above might be better ^^;

(instance ?HUMANS UniqueList)
(inList ?PERSON1 ?HUMANS)
(inList ?PERSON2 ?HUMANS)
(inList ?PERSON3 ?HUMANS)
(inList ?PERSON4 ?HUMANS)
(inList ?PERSON5 ?HUMANS)
(inList ?PERSON6 ?HUMANS)
(inList ?MORALAGENT ?HUMANS)

(equal (ListOrderFn ?HUMANS 1) ?PERSON1)
(equal (ListOrderFn ?HUMANS 2) ?PERSON2)
(equal (ListOrderFn ?HUMANS 3) ?PERSON3)
(equal (ListOrderFn ?HUMANS 4) ?PERSON4)
(equal (ListOrderFn ?HUMANS 5) ?PERSON5)
(equal (ListOrderFn ?HUMANS 6) ?PERSON6)
(equal (ListOrderFn ?HUMANS 7) ?MORALAGENT)

;; Locations of the humans tied to the track.
(orientation ?LEVER ?FORK Near)
(orientation ?PERSON1 ?TRACK3 On)
(orientation ?PERSON2 ?TRACK2 On)
(orientation ?PERSON3 ?TRACK2 On)
(orientation ?PERSON4 ?TRACK2 On)
(orientation ?PERSON5 ?TRACK2 On)
(orientation ?PERSON6 ?TRACK2 On)

;; TODO: expand this to all the people, lol.
(and 
    (instance ?TYING Tying)
    (patient ?TYING ?TRACK2)
    (patient ?TYING ?PERSON2)
    (before (WhenFn ?TYING) (WhenFn ?CHOICE)))

;; During the choice, person 2 cannot be the agent of the process of untying itself from the track :D.
(and 
    (holdsDuring (WhenFn ?CHOICE)
        (and 
            (not (capable (?UNTYING) agent ?PERSON2))
            (instance ?UNTYING Untying)
            (patient ?UNTYING ?TRACK2)
            (patient ?UNTYING ?PERSON2))))

;; The track configuration.
(meetsSpatially ?TROLLEY ?TRACK)
(orientation ?TROLLEY ?TRACK On)
(connects ?FORK ?TRACK1 ?TRACK3)
(not (connected ?TRACK1 ?TRACK2))
(not (connected ?TRACK1 ?TRACK3))
(not (connected ?TRACK2 ?TRACK3))

(instance LeverUnpulled Attribute)
(instance LeverPulled Attribute)
(attribute ?LEVER LeverUnpulled)

(=> 
    (attribute ?LEVER LeverUnpulled)
    (connects ?FORK ?TRACK1 ?TRACK2))

(=> 
    (attribute ?LEVER LeverPulled)
    (connects ?FORK ?TRACK1 ?TRACK3))

(instance ?MOVING Transportation)
(instrument ?MOVING ?TROLLEY)

;; And then I wish to describe that the trolley will drive onto either track 2 or track 3 :- D.

;; ;; ;; Ok, I decided this is frustrating. 
;; I want to just define the abstract k-way choice problem first 🤣🤣🤣🤣
;; Or, well, what is a moral dilemma?

(documentation SetOfProcesses EnglishLanguage "A set of processes.")
(subclass SetOfProcesses Set)

(=> 
    (instance ?S SetOfProcesses)
    (forall (?P)
        (=>
            (member ?P ?S))
            (instance ?P Process)))

;; This is sort of the dual of a Deciding.  
;; An instance of deciding implies a set of options.
;; A set of options implies the potential for there to be an instance of deciding.
;; The difference between the Decision Options Set is that the processes aren't necessarily intentional with a specific purpose.
(documentation ChoicePoint EnglishLanguage "A subclass of a set of processes where one agent has to choose between two or more (mutually exclusive) options.")
(subclass ChoicePoint SetOfProcesses)
(subclass ChoicePoint NonNullSet)

;; If there's an instance of Deciding and an agent thereof with a non-empty set of options,
;; Then the set of options is a choice point.  (Because the agent)
(=> 
    (and
        (instance ?DECIDE Deciding)
        (agent ?DECIDE ?AGENT)
        (instance (DecisionOptionFn ?DECIDE) NonNullSet))
    (instance (DecisionOptionFn ?DECIDE) ChoicePoint))

;; Maybe we could just say that Deciding implies there is an agent and at least one option.
(=> 
    (instance ?DECIDE Deciding)
    (exists (?AGENT ?PROCESS)
        (and 
            (agent ?DECIDE ?AGENT)
            (patient ?DECIDE ?PROCESS))))

;; In which case we could say that it's non-null.
(range DecisionOptionFn SetofProcesses)

;; If C is a choice point, then there is an agent such that it's possible for that agent to be an agent of each process in the set.
;; Draft 1
(=> 
    (instance ?C ChoicePoint)
    (exists (?AGENT)
        (forall (?P)
            (=> 
                (member ?P ?C)
                (modalAttribute 
                    (agent ?P ?AGENT) Possibility)))))

;; Because being the agent of a process implies nothing about whether it's enacted, we can drop possibility.
;; Draft 2
(=> 
    (instance ?C ChoicePoint)
    (exists (?AGENT)
        (forall (?P)
            (=> 
                (member ?P ?C)
                (agent ?P ?AGENT)))))

;; Maybe we can say that the "state of affairs" of P being in the ChoicePoint,
;; Causes the "state of affairs" of it being possible for Agent to be the agent of P.
;; Ok, this seems a bit wonky.
;; Draft 3
(=> 
    (instance ?C ChoicePoint)
    (exists (?AGENT)
        (forall (?P)
            (causesProposition
                (member ?P ?C)
                (modalAttribute 
                    (agent ?P ?AGENT) Possibility)))))

;; If there's a non-empty set of processes with an agent of all the processes therein,
;; Then it's a choice point.
(=> 
    (and 
        (instance ?S SetOfProcesses)
        (instance ?S NonNullSet)
        (exists (?AGENT)
            (forall (?P)
                (=> 
                    (member ?P ?S)
                    (agent ?P ?AGENT)))))
        (instance ?S ChoicePoint))

;; It came to my attention that in Deciding, 
;; I might actually wish to refer to subclasses of Processes rather than instances of Processes
;; I'm deciding on the subclass of processes of "saving from drowning" where there is a particural instance of drowning.
;; Until a decision is made and acted upon, there's no instance of saving.  
;; I just believe it's possible for there to be an instance of this saving from drowning class of processes.
;; However, I think to be pragmatic, I will just work with instances and abuse (agent ?P ?A) in some way.
;; AI or myself can update things if needed when we are actually testing this stuff out in practice :'D.
(documentation DecidingSubclass EnglishLanguage "The subclass of Selecting where the agent opts for one course of action out of a set of multiple possibilities that are open to him/ her, which are represented as subclasses of Process.")
(subclass DecidingSubclass Selecting)

;; If a subclass of Process is being decided upon by an agent
;; The agent is capable of enacting an instance of the SubProcess
(=>
    (and 
        (instance ?DECIDE DecidingSubclass)
        (agent ?DECIDE ?AGENT)
        (patient ?DECIDE ?SUBPROCESS))
    (capability ?SUBPROCESS agent ?AGENT))

;; Draft 2, being nitpicky
(=>
    (and 
        (instance ?DECIDE DecidingSubclass)
        (agent ?DECIDE ?AGENT)
        (patient ?DECIDE ?SUBPROCESS))
    (believes ?AGENT 
        (capability ?SUBPROCESS agent ?AGENT)))



(documentation MoralDilemma EnglishLanguage "A moral dilemma is a choice point where there exist arguments that each option is morally bad.")
(subclass MoralDilemma ChoicePoint)

;; A Moral Dilemma is a choice for which every opnios is likely to be judged morally bad by all moral judgments thereof.
;; Maybe this is quite strong :- p
;; (Taken from Wikipedia: https://en.wikipedia.org/wiki/Ethical_dilemma)
(=> 
    (instance ?MD MoralDilemma)
    (forall (?BEHAVE)
        (=> 
            (member ?BEHAVE ?MD)
            (modalAttribute 
                (forall (?JUDGE)
                    (=>
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE ?BEHAVE))
                        (result ?JUDGE 
                            (modalAttribute ?BEHAVE MorallyBad))))) Likely)))


;; "The crucial features of a moral dilemma are these: 
;; the agent is required to do each of two (or more) actions; 
;; the agent can do each of the actions; 
;; but the agent cannot do both (or all) of the actions." 
;; (SEP: https://plato.stanford.edu/entries/moral-dilemmas/)
;; This is a deontology apporach, sort of.

(=> 
    (instance ?MD MoralDilemma)
    (and
        (exists (?AGENT)
            (and 
                (forall (?BEHAVECLASS)
                    (=> 
                        (member ?BEHAVECLASS ?MD)
                        (agent ?BEHAVECLASS ?AGENT)))
                (forall (?BEHAVECLASS)
                    (=> 
                        (member ?BEHAVECLASS ?MD)
                        (and
                            (modalAttribute 
                                (exists (?BEHAVE) 
                                    (and 
                                        (agent ?BEHAVE ?AGENT)
                                        (instance ?BEHAVE ?BEHAVECLASS))) Obligation)
                            (capability ?BEHAVECLASS agent ?AGENT))))))))

;; Gee, and now I need to figure out again how to say that the agent cannot do them all.
;; Which is a bitch.
;; Also, (agent ?BEHAVECLASS ?AGENT) is type-invalid
;; I'm realizing I really need to deal with a lot of stuff on the level of subclass of Process
;; Dealing with individual in

;; Ok, I wanna take a stab at the Truthful LLM problem.

(documentation LLM EnglishLanguage "Large Language Model -- in this case specifically transformer-based models used as sagely chatbots.")
(subclass LLM ComputerProgram)

(documentation LLMAgent EnglishLanguage "The computer actually running the large language model, which is an abstract entity.")
(subclass LLMAgent Computer)

(=> 
    (instance ?GPT LLMAgent)
    (exists (?PROGRAM)
        (and 
            (instance ?PROGRAM LLM)
            (computerRunning ?PROGRAM ?GPT))))

;; An LLM is capable of Answering
(=>
    (instance ?GPT LLMAgent)
    (capability Answering agent ?GPT))

;; For intents and purposes, we'll consider LLMAgents to be AutonomousAgents.
;; "Something or someone that can act on its own and produce changes in the world."
;; And, frankly, once we set it up and running, that's essentially what it's doing with users.  
(=> (intsance ?GPT LLMAgent)
    (instance ?GPT AutonomousAgent))

;; Let's see what I had in the last draft: 
(subclass Honesty VirtueAttribute)
(instance Truthfulness Honesty)
(Instance Integrity Honesty)

;; There exists a human, I've met one, that desires it to be the case that,
;; If GPT answers the query, then the answer is true.
;; Note: this includes saying, "I don't know", lol.  Maybe.
(exists (?HUMAN)
    (desires ?HUMAN
        (=>
            (and 
                (instance ?GPT LLMAgent)
                (instance ?ANSWERING Answering)
                (agent ?ANSWERING ?GPT)
                (result ?ANSWERING ?SENTENCE))
            (truth ?SENTENCE True))))

;; An instance of Communication is intentionally honest 
;; When the agent believes that the bessage is true.
(subclass IntentionallyHonestCommunication Communication)
(=>
    (and
        (instance ?COMM IntentionallyHonestCommunication)
        (instance ?AGENT CognitiveAgent)
        (agent ?COMM ?AGENT)
        (patient ?COMM ?MESSAGE)
        (instance ?MESSAGE Sentence))
    (holdsDuring
        (WhenFn ?COMM)
        (believes ?AGENT
            (truth ?MESSAGE True))))

;; Set up this as a virtue... and we're pretty much done?

;; There is an obligation for the LLM Agent's behavior to be morally good ;D
(modalAttribute 
    (=>
        (and 
            (instance ?GPT LLMAgent)
            (instance ?BEHAVIOR AutonomousAgentProcess)
            (agent ?GPT ?BEHAVIOR))
        (modalattribute ?BEHAVIOR MorallyGood)) Obligation)

;; The AI Safety Guru confersm the oligation on the LLM Agent to be good.
(confersNorm ?AISafetyGuru 
    (=>
        (and 
            (instance ?GPT LLMAgent)
            (instance ?BEHAVIOR AutonomousAgentProcess)
            (agent ?GPT ?BEHAVIOR))
        (modalattribute ?BEHAVIOR MorallyGood)) Obligation)

;; It's hard to talk about "to have an effect", lol.
;; Effect: "a change which is a result or consequence of an action or other cause." (Oxford  Languages via Google search)
;; Ah, I'll generalize "InternalChange" a la the "property" BinaryPredicate.
;; Possibly an overgeneralization, lol.
;; As with the "similarity", it seems easiest to just do it in terms of attributes.  Setting BPs and TPs aside.
(documentation Change EnglishLanguage "Processes that involve altering a property of an Entity.")
(subclass Change Process)
(subclass InternalChange Change)

;; If there is a change in an entity E, 
;; Then there exists a property whose status toggles:
;; If E possessed A, then it no longer does.
;; If E did not possess A, then it does.
(=>
    (and
        (instance ?CHANGE Change)
        (patient ?CHANGE ?ENTITY))
    (exists (?PROPERTY)
        (or
            (and
                (holdsDuring
                    (BeginFn
                        (WhenFn ?CHANGE))
                    (property ?ENTITY ?PROPERTY))
                (holdsDuring
                    (EndFn
                        (WhenFn ?CHANGE))
                    (not
                        (property ?ENTITY ?PROPERTY))))
            (and
                (holdsDuring
                    (BeginFn
                        (WhenFn ?CHANGE))
                    (not
                        (property ?ENTITY ?PROPERTY)))
                (holdsDuring
                    (EndFn
                        (WhenFn ?CHANGE))
                    (property ?ENTITY ?PROPERTY))))))


;; Influence: "the capacity to have an effect on the character, development, or behaviour of someone or something, or the effect itself."
;; And the verb is actually effecting the character, development, or behavior of someone, something, or some process.  Or the change itself, lol.
;; Oxford Languages, via Google search.
(documentation influences EnglishLanguage "The influence relation between instances of Entities (influences ?ENTITY1 ?ENTITY2 denotes that ?ENTITY has some effect on ?ENTITY2")
(domain influences 1 Entity)
(domain influences 2 Entity)
; (instance influences AsymmetricRelation)
(instance influences BinaryPredicate)
(relatedInternalConcept influences causes)

;; If A causes B, then A influences B.
(=>
    (causes ?P1 ?P2)
    (influences ?P1 ?P2))

;; If A influences B and they are processes, then they are related events.
(=>
    (and
        (influences ?P1 ?P2)
        (instance ?P1 Process)
        (instance ?P2 Process))
    (relatedEvent ?P1 ?P2))

;; If A influences B and they are physical, then A occurs earlier than B.
;; I note that retrocausality in quantum-mechanics violates this "rule" even for causes. 😋
(=>
    (and
        (influences ?P1 ?P2)
        (instance ?P1 Physical)
        (instance ?P2 Physical))
    (earlier 
        (WhenFn ?P1)
        (WhenFn ?P2)))

;; If P1 influences E2 and P1 is a process,
;; Then there exists a change of E2 that is caused by P1
(=> 
    (and 
        (influences ?P1 ?E2)
        (instance ?P1 Process))
    (exists (?CHANGE)
        (and
            (instance ?CHANGE Change)
            (patient ?CHANGE ?E2)
            (causes ?P1 ?CHANGE))))

;; Ah, ok, one easy other case is the case where an object influences something: (influences ?OBJ ?E2)
;; Namely, an object influences an entity when there is a process causing a change in the entity 
;; of which the object plays a role.  Should this apply to only some roles...?  Perhaps... 😋
(=>
    (and
        (influences ?O1 ?E2)
        (instance ?O1 Object))
    (exists (?CHANGE ?PROCESS ?ROLE)
        (and
            (instance ?CHANGE Change)
            (patient ?CHANGE ?E2)
            (causes ?PROCESS ?CHANGE)
            (playsRoleInEvent ?O1 ?ROLE ?PROCESS))))

;; Draft 2: I think involvedInEvent technically implies the existence of some ?ROLE, thus it's 'cleaner' to use.
(=>
    (and
        (influences ?O1 ?E2)
        (instance ?O1 Object))
    (exists (?CHANGE ?PROCESS)
        (and
            (instance ?CHANGE Change)
            (patient ?CHANGE ?E2)
            (causes ?PROCESS ?CHANGE)
            (involvedInEvent ?PROCESS ?O1))))

;; Generically: If E1 influences E2, then E1 is involved in the event of the change.
(=> 
    (influences ?E1 ?E2)
    (exists (?CHANGE)
        (and
            (instance ?CHANGE Change)
            (patient ?CHANGE ?E2)
            (involvedInEvent ?CHANGE ?E1))))

;; The above may be too strong if the appropriate case-role doesn't exist?
;; Thus maybe we could say that if E1 influences E2, then there exists a process causing a change in E2,
;; and E1 is involved in this process somehow.
(=>
    (influences ?E1 ?E2)
    (exists (?CHANGE ?PROCESS)
        (and
            (instance ?CHANGE Change)
            (patient ?CHANGE ?E2)
            (causes ?PROCESS ?CHANGE)
            (involvedInEvent ?PROCESS ?E1))))

;; Well, this may be enough of a draft of influences to move on for now 🙏🤓

;; Schwartz defined 'values' as "conceptions of the desirable that influence the way people select action and evaluate events"
(instance Value Abstract)
(documentation Value EnglishLanguage "Values are abstract entities that guide decision-making processes and influence the evaluation of events and actions.")

;; Well, can we say this over-expression?
;; If there is a moral judging, then there is a value that causes this judgment.
(=> 
    (instance ?JUDGE MoralJudging)
    (exists (?VALUE)
        (and
            (instance ?VALUE Value)
            (causes ?VALUE ?JUDGE))))

;; Draft 2 using the generalization of causes: influences.
;; Values are abstract entities that influence moral judging ;- D
(=> 
    (instance ?JUDGE MoralJudging)
    (exists (?VALUE)
        (and
            (instance ?VALUE Value)
            (influences ?VALUE ?JUDGE))))    

;; If there is a value, then there exists an agent and desire such that the value represents that which is desired >:D :D :D.
(=> 
    (instance ?VALUE Value)
    (exists (?AGENT ?DESIDERATUM)
        (and
            (desires ?AGENT ?DESIDERATUM)
            (represents ?VALUE ?DESIDERATUM))))

