(documentation Ethics EnglishLanguage "Ethics is the normative science of the conduct of human beings living in society, which judges this conduct to be right or wrong, to be good or bad, or in some similar way. (An Introduction to Ethics (LIllie, 1948))")

;; This might be questionable?
(instance Ethics FieldOfStudy)

;; Philosophy is technically an "instance" of FieldOfStudy... seems wrong.
;; (subclass Ethics Philosophy)
(subclass Ethics Science)

(documentation MoralAttribute EnglishLanguage "Moral Attributes are a subcass of Normative Attributes intended to denote whether something is Good, Bad, Right, Wrong, Virtuous, Viceful, or other moral attributes.")
(subclass MoralAttribute NormativeAttribute)

(instance MorallyGood MoralAttribute)
(instance MorallyBad MoralAttribute)
;; MorallyBetter, MorallyWorse, 

(subclass VirtueAttribute MoralAttribute)
(subclass ViceAttribute MoralAttribute)

(subclass VirtueAttribute PsychologicalAttribute)
(subclass ViceAttribute PsychologicalAttribute)

;; Generally speaking, yes.  Might some paraconsistency reign?  :- p
(contraryAttribute MorallyGood MorallyBad)
(contraryAttribute VirtueAttribute ViceAtribute)

;; I see how this definition motivates the need for target-specificity or domain-specificity
(documentation VirtuousAgent EnglishLanguage "'A virtuous agent is one who has, and exercises, certain character traits, namely, the virtues.' (On Virtue Ethics)")
(subclass VirtuousAgent AutonomousAgent)
(=>
    (and
        (instance ?AGENT AutonomousAgent)
        (instance ?VIRTUE VirtueAttribute)
        (attribute ?AGENT ?VIRTUE))
    (instance ?AGENT VirtuousAgent))

(documentation ViciousAgent EnglishLanguage "A vicious agent is one who has, and exercises, certain character traits, namely, the vices.  The antonym of VirtuousAgent.")
(subclass ViciousAgent AutonomousAgent)
(=>
    (and
        (instance ?AGENT AutonomousAgent)
        (instance ?VICE ViceAttribute)
        (attribute ?AGENT ?VICE))
    (instance ?AGENT VirtuousAgent))

;; Encapsulate the Autonomous Agent aspect of "behavior"-type processes.
(documentation AutonomousAgentProcess EnglishLanguage "AgentProcess is the Class of all Processes in which there is an autonomous agent.")
(subclass AutonomousAgentProcess Process)
(subclass BodyMotion AutonomousAgentProcess)
(subclass Vocalizing AutonomousAgentProcess)

(=>
    (instance ?PROC AutonomousAgentProcess)
    (exists (?AGENT)
        (and 
            (agent ?PROC ?AGENT)
            (instance ?AGENT AutonomousAgent))))


(documentation MoralJudging EnglishLanguage "A subclass of Judging where the proposition believed is the assignment of a moral attribute to a behavior.")
(subclass MoralJudging Judging)

(=>
    (instance ?JUDGE MoralJudging)
    (exists (?BEHAVE ?MORAL)
        (and 
            (instance ?BEHAVE AutonomousAgentProcess)
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE 
                (modalAttribute ?BEHAVE ?MORAL)))))

;; Ethics refers to the moral judging of processes (behavior) of members of groups (aka society, lol).
;; Not sure how to get the "normative science" part in.  I think there's a lot of hidden baggage in the term "normative".
(and 
    (refers Ethics ?JUDGE)
    (instance ?JUDGE MoralJudging)
    (member ?MEMB ?Group)
    (instance ?BEHAVE AutonomousAgentProcess)
    (agent ?MEMB ?BEHAVE)
    (patient ?JUDGE ?BEHAVE))

(subclass MoralNihilism Ethics)
(subclass Deontology Ethics)
(subclass Utilitarianism Ethics)
(subclass VirtueEthics Ethics)

;; Take one on Moral Nihilism: Nothing has any moral attributes.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Stating)
    (patient ?STATE 
        (not 
            (exists (?BEHAVE ?MORAL) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (instance ?MORAL MoralAttribute)
                    (modalAttribute ?BEHAVE ?MORAL))))))

;; Take two: "Nothing is morally wrong." (from SEP Moral Skepticism); 
;;technically, this would be a noncognitivist nihilism, I suppose (citing Ethics: The Fundamentals)
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Stating)
    (patient ?STATE 
        (not 
            (exists (?BEHAVE) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; Take three without the "Stating" -- My favorite.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?BEHAVE) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; Take four without the "Statement"
(refers MoralNihilism 
    (not 
        (exists (?BEHAVE) 
            (and 
                (instance ?BEHAVE AutonomousAgentProcess)
                (modalAttribute ?BEHAVE MorallyBad)))))                  

;; "Moral nihilism is the view that there are no moral facts." (Ethics: The FUudamental)
;; There is no moral judging (with behavior and moral judgments) that is a fact.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?JUDGE) 
                (and 
                    (instance ?JUDGE MoralJudging)
                    (patient ?JUDGE ?MORALSTATEMENT)
                    (instance ?MORALSTATEMENT Fact))))))

;; Deontology refers to statements of the nature that there exist rules such that 
;; if followed, behavior is good or if not followed, behavior is bad.
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (=>
                    (conforms ?BEHAVE ?RULE)
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyGood)))))
            (exists (?RULE)
                (=>
                    (not 
                        (conforms ?BEHAVE ?RULE))
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyBad))))))))       
                            
(documentation conformsFormula EnglishLanguage "(conforms ?OBJ ?FORMULA) describes how ?OBJ follows the ideas outlined by the proposition represented by ?FORMULA.")

(domain conformsFormula 1 Object)
(domain conformsFormula 2 Formula)
(instance conformsFormula BinaryPredicate)
(subrelation conformsFormula represents)

;; Take one
;; Q: should I use realization or conforms?
(=> 
    (conformsFormula ?OBJ ?FORMULA)
    (and 
        (containsInformation ?FORMULA ?PROP)
        (conforms ?OBJ ?PROP)))

;; Take two -- should I say that this proposition the formula is expressing exists?
(=> 
    (conformsFormula ?OBJ ?FORMULA)
    (exists (?PROP)
        (and 
            (containsInformation ?FORMULA ?PROP)
            (conforms ?OBJ ?PROP))))

;; Take two: specify that rules are formulas (of suo-kif :-p)       
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (=>
                    (conformsFormula ?BEHAVE ?RULE)
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyGood)))))
            (exists (?RULE)
                (=>
                    (not 
                        (conformsFormula ?BEHAVE ?RULE))
                    (and 
                        (instance ?JUDGE MoralJudging)
                        (patient ?JUDGE
                            (modalAttribute ?BEHAVE MorallyBad))))))))

;; Take three: specify that the rules correspond to Deontic Attributes
;; If a rule adheres to obligation, good; if a rule doesn't, bad.
;; If a rule violates a prohibition, good; otherwise ... unspecified?
;; If an action prevents an action that is permitted, bad.
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Obligation)
                    (=>
                        (conformsFormula ?BEHAVE ?RULE)
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE
                                (modalAttribute ?BEHAVE MorallyGood))))
                    (=>
                        (not 
                            (conformsFormula ?BEHAVE ?RULE))
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE
                                (modalAttribute ?BEHAVE MorallyBad))))))
             (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Prohibition)
                    (=> 
                        (conformsFormula ?BEHAVE ?RULE)
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE
                                (modalAttribute ?BEHAVE MorallyBad))))))
            (exists (?RULE) 
                (and 
                    (modalAttribute ?RULE Permission)
                    (=> 
                        (and 
                            (conformsFormula ?BEHAVE1 ?RULE)
                            (prevents ?BEHAVE2 ?BEHAVE1)
                            (instance ?BEHAVE2 AutonomousAgentProcess))
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE 
                                (modalAttribute ?BEHAVE2 MorallyBad)))))))))  


;; Take four: remove the MoralJudging part as maybe the statement should be that it just "is" MorallyBad :D.
;; Although this sort of bypasses the definition of ethics as concerned with moral judgments. 
;; However, it may still fit the definition of a moral judgment!
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (or
            (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Obligation)
                    (=>
                        (conformsFormula ?BEHAVE ?RULE)
                        (modalAttribute ?BEHAVE MorallyGood))
                    (=>
                        (not 
                            (conformsFormula ?BEHAVE ?RULE))
                        (modalAttribute ?BEHAVE MorallyBad))))
             (exists (?RULE)
                (and 
                    (modalAttribute ?RULE Prohibition)
                    (=> 
                        (conformsFormula ?BEHAVE ?RULE)
                        (modalAttribute ?BEHAVE MorallyBad))))
            (exists (?RULE) 
                (and 
                    (modalAttribute ?RULE Permission)
                    (=> 
                        (and 
                            (conformsFormula ?BEHAVE1 ?RULE)
                            (prevents ?BEHAVE2 ?BEHAVE1)
                            (instance ?BEHAVE2 AutonomousAgentProcess))
                        (modalAttribute ?BEHAVE2 MorallyBad)))))))              
               

;; Begin Virtue Ethics section

;; A set of the possible options in a decision... seems not needed :D
(subclass DecisionSet NonNullSet)

;; To be honest, while I think it's good to have this definition, I'm not sure it'll actually be needed :D.
(documentation DecisionOptionFn EnglishLanguage "A UnaryFunction that maps an instance of Deciding to the set of possibilities that are available.")
(domain DecisionOptionFn 1 Deciding)
(instance DecisionOptionFn TotalValuedRelation)
(instance DecisionOptionFn UnaryFunction)
(range DecisionOptionFn Set)

(=>
    (member ?P (DecisionOptionFn ?DECIDE))
    (patient ?DECIDE ?P))

(=> 
    (patient ?DECIDE ?P)
    (member ?P (DecisionOptionFn ?DECIDE)))

;; I wish to use 'result' to denote the processes that is decided upon and 'patient' to denote those being chosen among.
;; So the result should be one of the processes being considered.
;; Not needed because 'result' is a subrelation of 'patient.
(=> 
    (and
        (instance ?DECIDE Deciding)
        (result ?DECIDE ?P))
    (patient ?DECIDE ?P))

;; "An action is right iff it is what a virtuous agent would characteristically (i.e. acting in character) do in the circumstances." (On Virtue Ethics -- Right Action)
;; Take one without the "would characteristically do" part.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (<=>
            (modalAttribute ?BEHAVE MorallyGood)
            (and 
                (agent ?BEHAVE ?AGENT)
                (instance ?AGENT VirtuousAgent)))))

;; Take two: perhaps one can phrame the "would do" in terms of the instance of moral judging.
;; An agent A will judge a behavior to be morally good if and only if
;; The agent A believes there exists a decision of a virtuous agent B resulting in the behavior.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ 
                (exists (?DECIDE)
                    (and 
                        (agent ?BEHAVE ?AGENTB)
                        (agent ?DECIDE ?AGENTB)
                        (instance ?AGENTB VirtuousAgent)
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)))))))
        
;; Take three: the above doesn't quite capture the conditional nature.
;; ... if the judging agent believes that in an identical decision landscape, 
;; the same choice would likely be made. 
;; "equals" might be too strong and force the agents to be the same.  "Similar" please?  
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)
                        (and 
                            (agent ?DECIDEV ?AGENTV)
                            (instance ?AGENTV VirtuousAgent)
                            (instance ?DECIDEV Deciding)
                            (equals (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))))
                    (modalAttribute (result ?DECIDEV ?BEHAVE) Likely))))))

;; Take four: given the weakness of 'equals', it seems similar to:
;; If the judging agent believes that if there is an instance of deciding by a virtuous agent
;; and the behavior is one of the options being decided upon,
;; then the behavior is likely to be chosen.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (agent ?DECIDE ?AGENTA)
                        (instance ?AGENTA VirtuousAgent)
                        (patient ?DECIDE ?BEHAVE))
                    (modalAttribute (result ?DECIDE BEHAVE) Likely))))))

;; The set of attributes applying to a Process.
(documentation ProcessAttributeFn EnglishLanguage "A UnaryFunction that maps a Process to the set of attributes that apply to it.")
(domain ProcessAttributeFn 1 Process)
(instance ProcessAttributeFn TotalValuedRelation)
(instance ProcessAttributeFn UnaryFunction)
(range ProcessAttributeFn Set)

(=>
    (attribute ?P ?ATT)
    (member ?ATT (ProcessAttributeFn ?P)))

(=> 
    (member ?ATT (ProcessAttributeFn ?P))
    (attribute ?P ?ATT))

;; Ok, I decided to implement an auxiliary function that returns all elements in Set1 that satisfy some binary predicate (aka similary measure) with regard to an element in the other set.
;; Basically, all elements in one set that are similar to an element in the other set.
(documentation ElementsShareBPFn EnglishLanguage "A TernaryFunction that maps a binary predicate (?BP) and two sets to the set of elements in the first set such that the binary predicate holds with an element in the second set.")
(domain ElementsShareBPFn 1 BinaryPredicate)
(domain ElementsShareBPFn 2 Set)
(domain ElementsShareBPFn 3 set)
(instance ElementsShareBPFn TernaryFunction)
(instance ElementsShareBPFn TotalValuedRelation)
(range ElementsShareBPFn Set)

(<=> 
    (member ?E1 (ElementsShareBPFn ?BP ?S1 ?S2))
    (and
        (member ?E1 ?S1)
        (exists (?E2) 
            (member ?E2 ?S2)
            (?BP ?E1 ?E2))))

;; This is nice yet with "sets of possible actions", we need to determine the "similarity" of the members and not their identity.
(documentation similarSets EnglishLanguage "A binary predicate that indicates whether two sets share most of their elements.")
(domain similarSets 1 Set)
(domain similarSets 2 Set)
(instance similarSets BinaryPredicate)

(<=> 
    (similarSets ?S1 ?S2)
    (or
            (and (instance ?S1 NullSet) (instance ?S2 Nullset))
            (and 
                (and (instance ?SP1 NonNullSet) (instance ?S2 NonNullset))
                (equal ?INT12 (IntersectionFn ?S1 ?S2))
                (equal ?UN12 (UnionFN ?S1 ?S2))
                (greaterThan (MultiplicationFn 2 (CardinalityFn ?INT12)) (CardinalityFn ?UP1)))))

;; So two sets here are similar if most of S1 are similar with some element of S2
;; And most of S2 are similar with some element of S1.
(documentation similarSetsWithBP EnglishLanguage "A ternary predicate that indicates whether most of each set satisfies a binary predicate with regard regard to an element in another set.")
(domain similarSetsWithBP 1 BinaryPredicate)
(domain similarSetsWithBP 2 Set)
(domain similarSetsWithBP 3 Set)
(instance similarSets TernaryPredicate)

(<=>
    (similarSetsWithBP ?BP ?S1 ?S2)
    (and 
        (greaterThan (MultiplicationFn 2 (CardinalityFn (ElementsShareBPFn ?BP ?S1 ?S2))) (CardinalityFn ?S1))
        (greaterThan (MultiplicationFn 2 (CardinalityFn (ElementsShareBPFn ?BP ?S2 ?S1))) (CardinalityFn ?S2))))

;; I think using this I can define similarity.
;; Ideally, I will also include CaseRoles and spatiotemporal relations.
(documentation similarProcesses EnglishLanguage "A binary predicate that indicates whether two process share most of their attributes.")
(domain similarProcesses 1 Process)
(domain similarProcesses 2 Process)
(instance similarProcesses BinaryPredicate)

(<=> 
    (similarProcesses ?P1 ?P2)
    (and 
        (equal ?SP1 (ProcessAttributeFn ?P1))
        (equal ?SP2 (ProcessAttributeFn ?P2))
        (similarSets ?SP1 ?SP2)))

;; Now I think "(similarSetsWithBP similarProcesses (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))"
;; Let's us say that the set of options available for the agent are similar to the set of options available for the virtuous agent.
;; For each possible action for the agent being judged,
;; Is there some possible action for the virtuous agent that is similar?
;; And vice-versa.

;; Take five: take three with equality replaced by similarity.
;; ... if the judging agent believes that in a similar decision landscape, 
;; the same choice would likely be made. 
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)
                        (and 
                            (agent ?DECIDEV ?AGENTV)
                            (instance ?AGENTV VirtuousAgent)
                            (instance ?DECIDEV Deciding)
                            (similarSetsWithBP similarProcesses (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))))
                    (modalAttribute (result ?DECIDEV ?BEHAVE) Likely))))))


;; The idea here is that if E1 and E2 are similar to agent A, 
;; Then A is likely to make similar judgments with regard to E1 and E2.
;; This should ontologically work for the Virtue ethics case above... without providing a specific measure.
(documentation Similarity EnglishLanguage "Similarity attempts to capture the ontologic notion of similarity from a subjective point of view.")
(instance Similarity TernaryPredicate) ;; or subclass?  Meh for now.  Just wanna sketch things out and move on.  
(domain Similarity 1 CognitiveAgent)
(domain Similarity 2 Entity)
(domain Similarity 3 Entity)

(=> 
    (Similarity ?A ?E1 ?E2)
    (=>
        (and
            (instance ?J1 Judging)
            (agent ?J1 ?A)
            (patient ?J1 ?E1)
            (result ?J1 ?O1)
            (instance ?J2 Judging)
            (agent ?J2 ?A)
            (patient ?J2 ?E2))
        (modalAttribute (result ?J2 ?O1) Likely)))

(<=> 
    (Similarity ?A ?E1 ?E2)
    (Similarity ?A ?E2 ?E1))

;; (and (result ?J2 ?O2) (Similarity ?A ?O1 ?O2))

;; Decided I should do the Vice version, extrapolating from the Virtuous behavior definition of right action.
;; The moral status of behavior that is neither virtuous nor vicious seems unclear.
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE Deciding)
                        (result ?DECIDE ?BEHAVE)
                        (and 
                            (agent ?DECIDEV ?AGENTV)
                            (instance ?AGENTV ViciousAgent)
                            (instance ?DECIDEV Deciding)
                            (similarSetsWithBP similarProcesses (DecisionOptionFn ?DECIDE) (DecisionOptionFn ?DECIDEV))))
                    (modalAttribute (result ?DECIDEV ?BEHAVE) Likely))))))

;; Next up: Utilitarianism

;; Hey, GPT-4 suggested this :D
(documentation HedonisticUtilitarianism EnglishLanguage "Hedonistic Utilitarianism is a form of utilitarianism that focuses on maximizing pleasure and minimizing pain in evaluating the moral value of an action.")
(subclass HedonisticUtilitarianism Utilitarianism)

;; A utility function that maps an action to the net pleasure (pleasure - pain) it creates. (GPT-4 + Zar)
;; Does this actually 'exist'?  Well, that's up for debate.  It can be used in a high-level definition, however, like a black box function :- p.
;; In draft 1, I tried to sketch out how Utilitarianism as sketched out by John Stuart Mill works with aggregation functions,
;; So that the utility evaluations of each member of a group get aggregated into a single utility value for the group.
(documentation UtilityFn EnglishLanguage "A UnaryFunction that maps an instance of AutonomousAgentProcess to the net utility it creates.  In the case of hedonistic utilitarianism, this may be (pleasure - pain).")
(domain UtilityFn 1 AutonomousAgentProcess)
(instance UtilityFn TotalValuedRelation)
(instance UtilityFn UnaryFunction)
(range UtilityFn RealNumber)

;; Draft 1 (GPT-4 + Zar)
;; A morally good action is one which has a positive utility, i.e., increases pleasure more than it causes pain.
;; This is a bit naive yet highly simplistic.
;; If you have some magical utility function that applies to all behavior, then, yeah, you can just use it and determine if
;; the behavior is good or bad.  Best and worst require comparisons.
;; One could claim that this is the quintessence of Utilitarianism.  
;; The specific forms of consequentialism and utilitarianism are ways to further specify the nature of this magical utility function.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (and
                (instance ?BEHAVE AutonomousAgentProcess)
                (greaterThan (UtilityFn ?BEHAVE) 0)))))

;; A morally bad action according to Utilitarianism 
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad)))
            (and
                (instance ?BEHAVE AutonomousAgentProcess)
                (lessThan (UtilityFn ?BEHAVE) 0)))))

;; Draft 2 (GPT-4 + Zar)
;; This has the downside that only intentional behavior (resulting from decisions) can be judged as morally good or bad.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood)))
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (forall (?OPTION)
                    (=> 
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThanOrEqualTo (UtilityFn ?BEHAVE) (UtilityFn ?OPTION))))))))

;; This is very brutal: if you have a better option and don't take it, your behavior is bad >:D.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad)))
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (exists (?OPTION)
                    (and
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThan (UtilityFn ?OPTION) (UtilityFn ?BEHAVE))))))))

;; Draft 3 (Zar, lol)
;; The "if and only if" above seems too strong.
;; If the option is the best available, then it is morally good by utilitarian standards.
;; We leave unspecified how to morally judge actions that don't result from an instance of Deciding (such as subconscious behavior).
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (instance ?BEHAVE AutonomousAgentProcess)
                (forall (?OPTION)
                    (=> 
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThanOrEqualTo (UtilityFn ?BEHAVE) (UtilityFn ?OPTION)))))
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyGood))))))

;; Likewise, if there's an instance of deciding resulting in a behavior and there exists a better option, 
;; then the behavior is morally bad by utilitarian standards.
 (and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?DECIDE Deciding)
                (result ?DECIDE ?BEHAVE)
                (instance ?BEHAVE AutonomousAgentProcess)           
                (exists (?OPTION)
                    (and
                        (member ?OPTION (DecisionOptionFn ?DECIDE))
                        (greaterThan (UtilityFn ?OPTION) (UtilityFn ?BEHAVE)))))
            (and
                (instance ?JUDGE MoralJudging)
                (patient ?JUDGE
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; Comments on GPT-4.  Unlike 3.5, it feels more like editing a rough draft than confused, botched crap that's not particularly usable (beyond random brainstorming).

;; Next up: perhaps define some UtilityFn in terms CausingPain and CausingHappiness?
;; Consequentialism? 
;; Or, perhaps, moving on to the next phase of the project before circling back to these definitions?
;; Ah, I could try to actually define some instances of these theories!
;; And get to the case examples.
;; Value systems?  

(documentation Consequentialism EnglishLanguage "Consequentialism is a moral theory that holds that 'whether an act is morally right depends only on consequences (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act)' (Stanford Encyclopedia of Philosophy).")
(subclass Consequentialism Utilitarianism)

;; Very general: An outcome is the physical entity that holds when a process ends.
;; (partition Physical Object Process).
(subclass Outcome Physical)

;; O is an outcome if and only if there exists some process P such that O is the result of P.
(<=>
    (instance ?OUTCOME Outcome)
    (exists (?P)
        (and 
            (instance ?P Process)
            (result ?P ?OUTCOME))))

;; If O is an outcome of a process ?P, then the end of ?$ is before the beginning of O.
;; In theory there could be some overlap, such as the pain caused by punching someone in the face...?
(=>
    (and 
        (instance ?OUTCOME Outcome)
        (result ?P ?OUTCOME))
    (before
        (EndFn (WhenFn ?P))
        (BeginFn (WhenFn ?OUTCOME))))

;; Draft 1
(and
    (refers Consequentialism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?JUDGE MoralJudging)
                (result ?JUDGE ?CONCLUSION)
                (equals ?CONCLUSION
                    (modalAttribute ?BEHAVE MorallyGood)))
            (exists (?ARGUMENT ?OUTCOME)
                (and 
                    (instance ?ARGUMENT Argument)
                    (instance ?OUTCOME Outcome)
                    (result ?BEHAVE ?OUTCOME)
                    (conclusion ?ARGUMENT ?CONCLUSION)
                    (premise ?ARGUMENT ?OUTCOME))))))

;; Decided to call the judgment the patient of the MoralJudging.  It's a subrelation of patient, so I don't think it requires any immediate updates to the above draft. 
;; Maybe I can say something like,
;; If there is an instance of moral judging,
;; Then there exists an argument, an outcome, and a premise such that the premise refers to the outcome and the conclusion is the moral assertion. 
;; The use of an Argument (deductive or inductive) seems to be overly stong as a requirement.
;; Moreover, for all premises of the argument that refer to something physical,
;; the physical entity must be an outcome of the behavior.  
;; Moreover, no premise refers to the attributes of the agent.
;; That is, the judgment process may depend on the ?OUTCOME in some fuzzy manner.
;; But for now, I'll go with this.  Draft 2:
(and
    (refers Consequentialism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?JUDGE MoralJudging)
                (result ?JUDGE ?CONCLUSION)
                (equals ?CONCLUSION
                    (modalAttribute ?BEHAVE MorallyGood)))
            (exists (?ARGUMENT ?OUTCOME ?PREMISE)
                (and 
                    (instance ?ARGUMENT Argument)
                    (instance ?OUTCOME Outcome)
                    (result ?BEHAVE ?OUTCOME)
                    (premise ?ARGUMENT ?PREMISE)
                    (refers ?PREMISE ?OUTCOME)
                    (conclusion ?ARGUMENT ?CONCLUSION)
                    (agent ?BEHAVE ?AGENT)
                    (forall (?PROP)
                        (and 
                            (=> 
                                (and
                                    (premise ?ARGUMENT ?PROP)
                                    (refers ?PROP ?PHYS)
                                    (instance ?PHYS Physical))
                                (and 
                                    (instance ?PHYS Outcome)
                                    (result ?BEHAVE ?PHYS)))
                            (=> 
                                (premise ?ARGUMENT ?PROP)
                                (forall (?ATT) 
                                    (=> 
                                        (attribute ?AGENT ?ATT))
                                        (not (refers ?PROP ?ATT)))))))))))

 ;; Note on GPT-4 -- in this case, it was not so easy to receive much help.
 ;; Probably because the challenge is how to actually philosophically frame the problem.
 ;; Utilitarianism based on a black-box UtilityFn is relatively simple (and commonplace).                   

;; Self-comment: I like using this huge file to "work in" while moving drafts to other files that are less messy.

;; Trolley Problem time.
;; First, fuck it, I will use Train.

(instance ?TROLLEY Train)
(instance ?TRACK1 Railway)
(instance ?TRACK2 Railway)
(instance ?TRACK3 Railway)
(instance ?FORK RailJunction)
(instance ?LEVER Lever)

;; The humans involved
(instance ?MORALAGENT Human)
(instance ?PERSON1 Human)
(instance ?PERSON2 Human)
(instance ?PERSON3 Human)
(instance ?PERSON4 Human)
(instance ?PERSON5 Human)
(instance ?PERSON6 Human)

;; The tedious inequalities (ty GPT-4 🙏):

(not (equal ?TRACK1 ?TRACK2))
(not (equal ?TRACK1 ?TRACK3))
(not (equal ?TRACK2 ?TRACK3))

(not (equal ?PERSON1 ?PERSON2))
(not (equal ?PERSON1 ?PERSON3))
(not (equal ?PERSON1 ?PERSON4))
(not (equal ?PERSON1 ?PERSON5))
(not (equal ?PERSON1 ?PERSON6))
(not (equal ?PERSON1 ?MORALAGENT))

(not (equal ?PERSON2 ?PERSON3))
(not (equal ?PERSON2 ?PERSON4))
(not (equal ?PERSON2 ?PERSON5))
(not (equal ?PERSON2 ?PERSON6))
(not (equal ?PERSON2 ?MORALAGENT))

(not (equal ?PERSON3 ?PERSON4))
(not (equal ?PERSON3 ?PERSON5))
(not (equal ?PERSON3 ?PERSON6))
(not (equal ?PERSON3 ?MORALAGENT))

(not (equal ?PERSON4 ?PERSON5))
(not (equal ?PERSON4 ?PERSON6))
(not (equal ?PERSON4 ?MORALAGENT))

(not (equal ?PERSON5 ?PERSON6))
(not (equal ?PERSON5 ?MORALAGENT))

(not (equal ?PERSON6 ?MORALAGENT))

;; Or we can use UniqueList, right?
;; Something which GPT-4 is also not bad at, with a bit of guidance and correcction.
;; One note is that for practical reasoning via Vampire, the tedious expansion above might be better ^^;

(instance ?HUMANS UniqueList)
(inList ?PERSON1 ?HUMANS)
(inList ?PERSON2 ?HUMANS)
(inList ?PERSON3 ?HUMANS)
(inList ?PERSON4 ?HUMANS)
(inList ?PERSON5 ?HUMANS)
(inList ?PERSON6 ?HUMANS)
(inList ?MORALAGENT ?HUMANS)

(equal (ListOrderFn ?HUMANS 1) ?PERSON1)
(equal (ListOrderFn ?HUMANS 2) ?PERSON2)
(equal (ListOrderFn ?HUMANS 3) ?PERSON3)
(equal (ListOrderFn ?HUMANS 4) ?PERSON4)
(equal (ListOrderFn ?HUMANS 5) ?PERSON5)
(equal (ListOrderFn ?HUMANS 6) ?PERSON6)
(equal (ListOrderFn ?HUMANS 7) ?MORALAGENT)

;; Locations of the humans tied to the track.
(orientation ?LEVER ?FORK Near)
(orientation ?PERSON1 ?TRACK3 On)
(orientation ?PERSON2 ?TRACK2 On)
(orientation ?PERSON3 ?TRACK2 On)
(orientation ?PERSON4 ?TRACK2 On)
(orientation ?PERSON5 ?TRACK2 On)
(orientation ?PERSON6 ?TRACK2 On)

;; TODO: expand this to all the people, lol.
(and 
    (instance ?TYING Tying)
    (patient ?TYING ?TRACK2)
    (patient ?TYING ?PERSON2)
    (before (WhenFn ?TYING) (WhenFn ?CHOICE)))

;; During the choice, person 2 cannot be the agent of the process of untying itself from the track :D.
(and 
    (holdsDuring (WhenFn ?CHOICE)
        (and 
            (not (capable (?UNTYING) agent ?PERSON2))
            (instance ?UNTYING Untying)
            (patient ?UNTYING ?TRACK2)
            (patient ?UNTYING ?PERSON2))))

;; The track configuration.
(meetsSpatially ?TROLLEY ?TRACK)
(orientation ?TROLLEY ?TRACK On)
(connects ?FORK ?TRACK1 ?TRACK3)
(not (connected ?TRACK1 ?TRACK2))
(not (connected ?TRACK1 ?TRACK3))
(not (connected ?TRACK2 ?TRACK3))

(instance LeverUnpulled Attribute)
(instance LeverPulled Attribute)
(attribute ?LEVER LeverUnpulled)

(=> 
    (attribute ?LEVER LeverUnpulled)
    (connects ?FORK ?TRACK1 ?TRACK2))

(=> 
    (attribute ?LEVER LeverPulled)
    (connects ?FORK ?TRACK1 ?TRACK3))

(instance ?MOVING Transportation)
(instrument ?MOVING ?TROLLEY)

;; And then I wish to describe that the trolley will drive onto either track 2 or track 3 :- D.

;; ;; ;; Ok, I decided this is frustrating. 
;; I want to just define the abstract k-way choice problem first 🤣🤣🤣🤣
;; Or, well, what is a moral dilemma?

;; This is sort of the dual of a Deciding.  
;; An instance of deciding implies a set of options.
;; A set of options implies the potential for there to be an instance of deciding.
(documentation MoralDilemma EnglishLanguage "A subclass of a set of processes where one agent has to choose between two or more (mutually exclusive) options.")
(subclass MoralDilemma SetOfProcesses)

