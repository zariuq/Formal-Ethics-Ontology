;; Draft 3 -- because dealing with moral judgments in terms of classes instead of instances 
;; seems substantial enough to warrant possibly revamping a lot!
;; I will not repeat unchanged elements from the Ethics Kifs folder.

(documentation DecidingSubclass EnglishLanguage "The subclass of Selecting where the agent opts for one course of action 
out of a set of multiple possibilities that are open to him/ her, which are represented as subclasses of Process, 
some instance of which can be enacted.")
(subclass DecidingSubclass Selecting)
(termFormat EnglishLanguage DecidingSubclass "deciding")

;; If a subclass of Process is being decided upon by an agent
;; The agent is capable of enacting an instance of the SubProcess
(=>
    (and 
        (instance ?DECIDE DecidingSubclass)
        (agent ?DECIDE ?AGENT)
        (patient ?DECIDE ?CPROCESS))
    (capability ?CPROCESS agent ?AGENT))

;; Draft 2, being nitpicky, maybe the agent only believes it's capable
;; And will decide on the action based on the belief without actually being able to ðŸ¤ª
(=>
    (and 
        (instance ?DECIDE DecidingSubclass)
        (agent ?DECIDE ?AGENT)
        (patient ?DECIDE ?CPROCESS))
    (believes ?AGENT 
        (capability ?CPROCESS agent ?AGENT)))

;; If an agent decides on a class of behaviors, then there exists an instance of this class of behaviors
;; that is an intentional process and of which the agent is an agent.
;; A bit of a stretch as agents don't strictly _need_ to act on their conclusions.
;; Just trying to sketch a "class" version of the "instance" Deciding in SUMO.
(=>
    (and
        (instance ?DECIDE DecidingSubclass)
        (agent ?DECIDE ?AGENT)
        (result ?DECIDE ?CPROCESS))
    (exists (?IPROCESS)
        (and
            (instance ?IPROCESS ?CPROCESS)
            (instance ?IPROCESS IntentionalProcess)
            (agent ?IPROCESS ?AGENT))))

;; Use time to say that the agent intends or believes that there will be such an instance after the time of deciding.

(=>
    (and
        (instance ?DECIDE DecidingSubclass)
        (agent ?DECIDE ?AGENT)
        (result ?DECIDE ?CPROCESS))
    (subclass ?CPROCESS IntentionalProcess))

;; While the above essentially imply that the patient of a Deciding(Subclass) is a subclass of Process
;; I'd like to make it clear.
(=>
    (and 
        (instance ?DECIDE DecidingSubclass)
        (patient ?DECIDE ?CPROCESS))
    (subclass ?CPROCESS IntentionalProcess))

;; The same for Deciding
(=>
    (and 
        (instance ?DECIDE Deciding)
        (patient ?DECIDE ?IPROCESS))
    (instance ?IPROCESS Process))

;; Messy but, whatever ;D. 
;; The below is what I'm using for the first SUMO pull request:
;; Whether to use believes or not is tricky.  It seems more 'proper'.
(=>
    (and 
        (instance ?DECIDE Deciding)
        (agent ?DECIDE ?AGENT)
        (patient ?DECIDE ?OPTION))
    (believes ?AGENT 
        (capability ?OPTION agent ?AGENT)))

;; So if one decides on a course of action,
;; Then one believes that there will be an instance of this action,
;; of which one is the agent that takes place after the decision.
(=>
    (and 
        (instance ?DECIDE Deciding)
        (agent ?DECIDE ?AGENT)
        (result ?DECIDE ?DECISION))
    (believes ?AGENT 
        (exists (?P)
          (and 
            (instance ?P ?DECISION)
            (agent ?P ?AGENT)
            (earlier
              (WhenFn ?DECIDE)
              (WhenFn ?P))))))

;; And one is deciding among intentional processes.
;; The existence of an agent is implied by the class-hierarchy, lol.
(=>
   (and
      (instance ?DECIDE Deciding)
      (patient ?DECIDE ?OPTION))
   (and
      (subclass ?OPTION IntentionalProcess))) 

;; I think this one is pretty much fine, lol.
;; Actually, it's a bit weird: it makes it seem as if one is choosing between subclasses of LegalDecision,
;; namely, LegalAcquittal, LegalAward, LegalConviction, LegalDismissal, and Sentencing.
;; One could stipulate that sometimes one is choosing among particular instances of a single class of procesess,
;; for exmaple, what sort of conviction to sentence someone to.  
;; A counter-argument would be that one is actually choosing among hitherto unspecified subclasses of LegalConviction.
;; Here, when there's a Legal Decision, there is an act of deciding on a type of legal decision that takes place earlier
;; than the decision.  And the decision is an instance of the type of legal decision decided upon.
;; Which, tbh, seems better than just dealing in instances.
;; (I should really look into what the intended semantics of instances are.  )

;; Anyway, i think it's good.  Better to include the connection between the decision and the decision. 
(=>
    (instance ?DECISION LegalDecision)
    (exists (?DECIDE)
        (and
            (instance ?DECIDE DecidingSubclass)
            (earlier
                (WhenFn ?DECIDE)
                (WhenFn ?DECISION))
            (result ?DECIDE ?LEGALDECISIONPROCESS)
            (instance ?DECISION ?LEGALDECISIONPROCESS))))

;; This shouldn't be deciding.
(=>
    (ratingsAgent ?RATING ?AGENT)
    (exists (?PROCESS)
        (and
            (instance ?PROCESS Deciding)
            (agent ?PROCESS ?AGENT)
            (result ?PROCESS ?RATING))))

;; So there are only two uses of Deciding in SUMO's KB anyway.
;; The rule in GameCall seems fine as it's vague.  Should probably be Judging.  I don't know enough about it.
;; Voting should perhaps be a subclass of Selecting, not Deciding
;; As one is often not actually selecting courses of action.
;; However, it would seem that all the uses of Voting and VotingFn do not refer to the result at all!
;; Thus all of the subclasses of Deciding can also be subclasses of DecidingSubclass!

;; Resolution could just as well be a subclass of Deciding!
(documentation ResolutionSubclass EnglishLanguage "Any instance of DecidingSubclass which is conducted at a FormalMeeting 
and where the agent is an Organization.")
(subclass ResolutionSubclass Decidingsubclass)

(=>
    (instance ?RESOLUTION ResolutionSubclass)
    (exists (?AGENT ?MEETING)
        (and
            (instance ?AGENT Organization)
            (agent ?RESOLUTION ?AGENT)
            (subProcess ?RESOLUTION ?MEETING)
            (instance ?MEETING FormalMeeting))))

;; I think that covers all the Decision: Instance -> Class update!  Yay ðŸ¥³
;; The first question is: what needs to be changed to suite the DecisionSubclass?

;; Fields of Study are now classes!
(documentation Ethics EnglishLanguage "Ethics is the normative science of the conduct of human beings living in society, 
which judges this conduct to be right or wrong, to be good or bad, or in some similar way. (An Introduction to Ethics (LIllie, 1948))")
(subclass Ethics Philosophy)
(subclass Ethics Science)

(documentation MoralNihilism EnglishLanguage "'Moral Nihilism is the view that nothing is morally wrong' (SEP - Moral Skepticism). 
Moral Nihilism can also be defined as 'the view that there are no moral facts' (Ethics: The Fundamentals).")
(subclass MoralNihilism Ethics)

(documentation Deontology EnglishLanguage "Deontology is the ethical paradigm that judges the morality of an action 
based on the action adheres to a set of rules and principles.")
(subclass Deontology Ethics)

(documentation Utilitarianism EnglishLanguage "Utilitarianism is the ethical paradigm that judges the morality of an action 
based on whether it maximizes the good over the bad, which is typically determined via a utility function.")
(subclass Utilitarianism Ethics)

(documentation VirtueEthics EngnlishLanguage "Virtue ethics is the ethical paradigm that judges the morality of an action 
based on the character of the agent performing an action.  A virtuous agent is one who possesses virtues.  
'An action is right if and only if it is what a virtuous agent would characteristically (i.e., acting in caharacter) 
do in the circumstances' (On Virtue Ethics -- Right Action).")
(subclass VirtueEthics Ethics)

(documentation HedonisticUtilitarianism EnglishLanguage "Hedonistic Utilitarianism is a form of utilitarianism that focuses on maximizing pleasure and minimizing pain in evaluating the moral value of an action.")
(subclass HedonisticUtilitarianism Utilitarianism)

(documentation Consequentialism EnglishLanguage "Consequentialism is a moral theory that holds that 'whether an act is morally right depends only on consequences (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act)' (Stanford Encyclopedia of Philosophy).")
(subclass Consequentialism Utilitarianism)

;; Repeated only because I added to them ;- )

;; MorallyGood, VirtueAttribute, VirtuousAgent and the vicious versions can remain the same âœ…
;; AutonomousAgentProcess is good.

; (=>
;     (instance ?JUDGE MoralJudging)
;     (exists (?BEHAVE ?MORAL)
;         (and 
;             (instance ?BEHAVE AutonomousAgentProcess)
;             (instance ?MORAL MoralAttribute)
;             (patient ?JUDGE ?BEHAVE)
;             (result ?JUDGE 
;                 (modalAttribute ?BEHAVE ?MORAL)))))
;; The above (draft 2) is wrong because because ?BEHAVE is a Process, not a Formula.  

;; If there is an instance ?J of Moral Judging, then there exists a class of autonomous agent behavior ?CB and a moral judgment ?M, 
;; such that it is judged to me ?M for there to exist an instance ?IB of ?CB.
;; -- This actually makes more sense!
(=>
    (instance ?JUDGE MoralJudging)
    (exists (?CBEHAVE ?MORAL)
        (and 
            (subclass ?CBEHAVE AutonomousAgentProcess)
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE ?CBEHAVE)
            (result ?JUDGE 
                (modalAttribute 
                    (exists (?IBEHAVE) 
                        (instance ?IBEHAVE ?CBEHAVE)) ?MORAL)))))

(modalAttribute 
(instance ?IBEHAVE ?CBEHAVE)) ?MORAL)

;; Isn't this just type inhabitation or something?
(documentation hasInstance EnglishLanguage "Auxiliary predicate to simplify definitions.")
(subclass hasInstance Predicate)
(domainSubclass hasInstance 1 Class)

(=> 
    (instance hasInstance ?REL)
    (valence ?REL 1))

(<=>
    (hasInstance ?CLASS)
    (exists (?INSTANCE)
        (instance ?INSTANCE ?CLASS)))

;; Using hasInstance: It's bad for there to be an instance of this class of autonomous behavior >:D.
(=>
    (instance ?JUDGE MoralJudging)
    (exists (?CBEHAVE ?MORAL)
        (and 
            (subclass ?CBEHAVE AutonomousAgentProcess)
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE ?CBEHAVE)
            (result ?JUDGE 
                (modalAttribute 
                    (hasInstance ?CBEHAVE) ?MORAL)))))

;;  We might wish to say that if there is a result of a moral judgment, then it is of the form that something judged morally.
(=> 
    (and
        (instance ?JUDGE MoralJudging)
        (result ?JUDGE ?RESULT))
    (exists (?JUDGED ?MORAL) 
        (equals ?RESULT
            (modalAttribute ?JUDGED ?MORAL))))

;; If there is an instance ?J of moral judging, then there exists an autonomous agent behavior ?B and a moral judgment ?M,
;; such that it is judged to be ?M that there is an instance of ?B.
;; -- A bit weird.  There's some precedence in SUMO.
;; I suppose we wish to get at the case where one is simply judging a particular behavior as GOOD or BAD without any clear sense of generalization. 
(=>
    (instance ?JUDGE MoralJudging)
    (exists (?BEHAVE ?MORAL)
        (and 
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE ?BEHAVE)
            (result ?JUDGE 
                (modalAttribute
                    (instance ?BEHAVE AutonomousAgentProcess) ?MORAL)))))

;; Maybe I wish to say that if there is a case of moral judging ?J, then it is either of the above cases:
;; Judging any instance of a class of behaviors ?M or judging a particular instance to be ?M.
(=>
    (instance ?JUDGE MoralJudging)
    (or
        (exists (?CBEHAVE ?MORAL)
        (and 
            (subclass ?CBEHAVE AutonomousAgentProcess)
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE ?CBEHAVE)
            (result ?JUDGE 
                (modalAttribute 
                    (exists (?IBEHAVE) 
                        (instance ?IBEHAVE ?CBEHAVE)) ?MORAL))))
        (exists (?BEHAVE ?MORAL)
        (and 
            (instance ?MORAL MoralAttribute)
            (patient ?JUDGE ?BEHAVE)
            (result ?JUDGE 
                (modalAttribute
                    (instance ?BEHAVE AutonomousAgentProcess) ?MORAL))))))

;; Ethics refers to the moral judging of processes (behavior) of members of groups (aka society, lol).
;; Not sure how to get the "normative science" part in.  I think there's a lot of hidden baggage in the term "normative".
(and 
    (refers Ethics ?JUDGE)
    (instance ?JUDGE MoralJudging)
    (instance ?MORAL MoralAttribute)
    (instance ?GROUP Group)
    (instance ?BEHAVE AutonomousAgentProcess)
    (member ?MEMB ?GROUP)
    (agent ?BEHAVE ?MEMB)
    (patient ?JUDGE ?BEHAVE)
    (result ?JUDGE 
        (modalAttribute ?BEHAVE ?MORAL)))
;; The above (draft 2) can be refined because the instance ?MORAL implies the behavior and the moral judgment.

;; This is tricky because I'm including both classes and instances.  Maybe I should drop instances for now.
;; Anyway, I wish to include the idea that the moral judgment is of a member of a group
(and 
    (refers Ethics ?JUDGE)
    (instance ?JUDGE MoralJudging)
    (instance ?GROUP Group)
    (member ?MEMB ?GROUP)
    (patient ?JUDGE ?BEHAVE)
    (=>
        (instance ?BEHAVE AutonomousAgentProcess)
        (agent ?BEHAVE ?MEMB))
    (=> 
        (subclass ?BEHAVE AutonomousAgentProcess)
        (capability ?BEHAVE agent ?MEMB)))

;; Let's add that the agent of the moral judgment is either an inclusive subgroup or a member of the group (with a bit of redundancy!)
(and 
    (refers Ethics ?JUDGE)
    (instance ?JUDGE MoralJudging)
    (instance ?GROUP Group)
    (member ?MEMB ?GROUP)
    (patient ?JUDGE ?BEHAVE)
    (=>
        (instance ?BEHAVE AutonomousAgentProcess)
        (agent ?BEHAVE ?MEMB))
    (=> 
        (subclass ?BEHAVE AutonomousAgentProcess)
        (capability ?BEHAVE agent ?MEMB))
    (agent ?JUDGE ?AGENT)
    (or
        (member ?AGENT ?GROUP)
        (part ?AGENT ?GROUP)))

;; I wish to add a minimalistic one that doesn't bother with the societal context
(and 
    (refers Ethics ?JUDGE)
    (instance ?JUDGE MoralJudging))

;; "Moral nihilism is the view that there are no moral facts." (Ethics: The Fundamentals)
;; There is no moral judging (with behavior and moral judgments) that is a fact.
;; Curiously, I think this doesn't need to be changed at all!
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?JUDGE) 
                (and 
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE ?MORALSTATEMENT)
                    (instance ?MORALSTATEMENT Fact))))))


;; "Nothing is morally wrong." (from SEP Moral Skepticism); 
;; technically, this would be a noncognitivist nihilism, I suppose (citing Ethics: The Fundamentals)
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?BEHAVE) 
                (and 
                    (instance ?BEHAVE AutonomousAgentProcess)
                    (modalAttribute ?BEHAVE MorallyBad))))))

;; There does not exist a class of behaviors for which there being an instance is morally bad.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (not 
            (exists (?CBEHAVE) 
                (and 
                    (subclass ?CBEHAVE AutonomousAgentProcess)
                    (modalAttribute (hasInstance ?CBEHAVE) MorallyBad))))))

;; Or we have the following phrasing: forall classes of behaviors, it's not the case that it's bad for there to be an instance thereof.
(and 
    (refers MoralNihilism ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE 
        (forall (?CBEHAVE)
            (=> 
                (subclass ?CBEHAVE AutonomousAgentProcess)
                (not 
                    (modalAttribute (hasInstance ?CBEHAVE) MorallyBad))))))                    


;; (conformsFormula ?BEHAVE ?RULE)
;; (conforms ?OBJ ?PROP)
;; (partition Physical Object Process)
;; (domain conforms 1 Object)
;; --> I cannot use conforms for this purpose.

; (=> 
;     (conformsFormula ?OBJ ?FORMULA)
;     (exists (?PROP)
;         (and 
;             (containsInformation ?FORMULA ?PROP)
;             (conforms ?OBJ ?PROP))))

;; Maybe rename this to realizes formula for natural language ease?
;; A process conforms to a formula if and only if there exists a proposition such that:
;; a) the formula contains the information of the proposition.
;; b) the process is the realization of the proposition.
(<=> 
    (realizesFormula ?PROCESS ?FORMULA)
    (exists (?PROP)
        (and 
            (containsInformation ?FORMULA ?PROP)
            (realization ?PROCESS ?PROP))))

;; A subclass of Process conforms to a formula if there exists a proposition such that:
;; a) the formula contains the information of the proposition.
;; b) all instances of the subclass are realizatinos of the proposition.
(<=> 
    (realizesFormulaSubclass ?CPROCESS ?FORMULA)
    (exists (?PROP)
        (and 
            (containsInformation ?FORMULA ?PROP)
            (forall (?IPROCESS)
                (=> 
                    (instance ?IPROCESS ?CPROCESS)
                    (realization ?IPROCESS ?PROP))))))

;; Probably the 'or' and the 'rule' should be swapped!
;; Maybe I want some auxiliary "Judges Behavior Morally Good" shorthand... some predicate?
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (exists (?RULE)
            (or
                (and 
                    (modalAttribute ?RULE Obligation)
                    (=>
                        (realizesFormula ?BEHAVE ?RULE)
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (modalAttribute ?BEHAVE MorallyGood))))
                    (=>
                        (not 
                            (realizesFormula ?BEHAVE ?RULE))
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (modalAttribute ?BEHAVE MorallyBad)))))
                (and 
                    (modalAttribute ?RULE Prohibition)
                    (=> 
                        (realizesFormula ?BEHAVE ?RULE)
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (modalAttribute ?BEHAVE MorallyBad)))))
                (and 
                    (modalAttribute ?RULE Permission)
                    (=> 
                        (and 
                            (realizesFormula ?BEHAVE1 ?RULE)
                            (prevents ?BEHAVE2 ?BEHAVE1)
                            (instance ?BEHAVE2 AutonomousAgentProcess))
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE 
                                (modalAttribute ?BEHAVE2 MorallyBad)))))))))

;; The next step is to make the class version.
;; One trick here is that the "class" aspect is less relevant here.
;; All I'm saying is that "if a behavior realizes the rule", "then it's good/bad".
;; But one thing that needs to be fixed is: a process is not a formula and thus modalAttribute cannot be applied to it.
;; I might also wish to make a version with confersNorm unifying the norm-confering agent with the morally judging agent!
;; I'm also changing the structure so that we separately state that the rule is either an obligation, prohibition, or permission.
;; And then we state what each case entails.
;; I think this works and we don't need to deal with the "class" aspect explicitly, for it's implicit in the existential quantification.
;; We probably wish to grapple with the notion of judging a whole class of behaviors to be good/bad tho.
;; Take one here is: if a rule is an obligation, then there exists a moral judgment
;; that it is good for there to exist an instance of a behavior realizing the rule.
;; and it is morally bad for there to NOT exist a behavior realizing the obligatory rule.
;; Before, dealing only with instances, I just said that it's bad if it doesn't satisfy the obligation (which would require a notion of temporality and mutual exclusivity).
;; With the existential claim, we seem quite a bit safer.
;; As before, the prohibition is simply the opposite of the obligation.
;; For permission, I'll say that there is a judgment such that for all classes of behaviors that realize the permission,
;; It is bad for there to exist an instance of behavior that prevents any of these classes from being instantiated.
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (exists (?RULE)
            (and
                (or
                    (modalAttribute ?RULE Obligation)
                    (modalAttribute ?RULE Prohibition)
                    (modalAttribute ?RULE Permission))
                (=>
                    (modalAttribute ?RULE Obligation)
                    (exists (?JUDGE)
                        (and
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (and 
                                    (modalAttribute 
                                        (exists (?BEHAVE)
                                            (and 
                                                (realizesFormula ?BEHAVE ?RULE)
                                                (instance ?BEHAVE AutonomousAgentProcess))) MorallyGood)
                                    (modalAttribute 
                                        (not 
                                            (exists (?BEHAVE)
                                                (and 
                                                    (realizesFormula ?BEHAVE ?RULE)
                                                    (instance ?BEHAVE AutonomousAgentProcess))) MorallyBad)))))))      
                (=>
                    (modalAttribute ?RULE Prohibition)
                    (exists (?JUDGE)
                        (and
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (modalAttribute 
                                        (exists (?BEHAVE)
                                            (and 
                                                (realizesFormula ?BEHAVE ?RULE)
                                                (instance ?BEHAVE AutonomousAgentProcess))) MorallyBad)))))
                (=> 
                    (modalAttribute ?RULE Permission)
                    (exists (?JUDGE)
                        (and
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (forall (CBEHAVE)
                                    (=> 
                                        (and
                                            (subclass ?CBEHAVE AutonomousAgentProcess)
                                            (realizesFormulaSubclass ?CBEHAVE ?RULE))                              
                                        (modalAttribute 
                                            (exists (?BEHAVE)
                                                (and
                                                    (instance ?BEHAVE AutonomousAgentProcess)
                                                    (prevents ?CBEHAVE ?BEHAVE))) MorallyBad)))))))))))
             
;;; I'll add the confersNorm part below.
;; Also, apparently Law is a subattribute of obligation, Legal of permission, and Illegal of prohibition.  
;; LegislativeBill (a proposed law) is the only outlier that should be fixed, probably
;; So there's an agent that declares this rule and this agent is the one that judges adherence to be good/bad.
;; Some deontological theories may not wish to make this distinction.  
;; I note that (domain confersNorm 1 Entity), so possibly one could say that the rule is conferred by mathematical necessity.
;; In which caset he agents doing the judging are distinct.
(and 
    (refers Deontology ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (exists (?RULE)
            (and
                (instance ?DEONTIC DeonticAttribute)
                (modalAttribute ?RULE ?DEONTIC)
                (exists (?AGENT) 
                    (confersNorm ?AGENT ?RULE ?DEONTIC))
                (=>
                    (modalAttribute ?RULE Obligation)
                    (exists (?JUDGE)
                        (and
                            (agent ?JUDGE ?AGENT)
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (and 
                                    (modalAttribute 
                                        (exists (?BEHAVE)
                                            (and 
                                                (realizesFormula ?BEHAVE ?RULE)
                                                (instance ?BEHAVE AutonomousAgentProcess))) MorallyGood)
                                    (modalAttribute 
                                        (not 
                                            (exists (?BEHAVE)
                                                (and 
                                                    (realizesFormula ?BEHAVE ?RULE)
                                                    (instance ?BEHAVE AutonomousAgentProcess))) MorallyBad)))))))      
                (=>
                    (modalAttribute ?RULE Prohibition)
                    (exists (?JUDGE)
                        (and
                            (agent ?JUDGE ?AGENT)
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (modalAttribute 
                                        (exists (?BEHAVE)
                                            (and 
                                                (realizesFormula ?BEHAVE ?RULE)
                                                (instance ?BEHAVE AutonomousAgentProcess))) MorallyBad)))))
                (=> 
                    (modalAttribute ?RULE Permission)
                    (exists (?JUDGE)
                        (and
                            (agent ?JUDGE ?AGENT)
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE
                                (forall (CBEHAVE)
                                    (=> 
                                        (and
                                            (subclass ?CBEHAVE AutonomousAgentProcess)
                                            (realizesFormulaSubclass ?CBEHAVE ?RULE))                              
                                        (modalAttribute 
                                            (exists (?BEHAVE)
                                                (and
                                                    (instance ?BEHAVE AutonomousAgentProcess)
                                                    (prevents ?CBEHAVE ?BEHAVE))) MorallyBad)))))))))))

;; Let's try to do a Classy version
; (and 
;     (refers Deontology ?STATE)
;     (instance ?STATE Statement)
;     (equal ?STATE
;         (exists (?RULE)
;             (and
;                 (instance ?DEONTIC DeonticAttribute)
;                 (modalAttribute ?RULE ?DEONTIC)
;                 (=>
;                     (and 
;                         (modalAttribute ?RULE Obligation)
;                         (exists (?CBEHAVE)
;                             (and 
;                                 (realizesFormulaSubclass ?CBEHAVE ?RULE))
;                                 (subclass ?CBEHAVE AutonomousAgentProcess)))
;                     ...
;                     ...
;                     ...
;; Actually, it's harder to make sense of how to do this.  
;; What I'm doing with the existence of an instance IS the class-sensitive approach. 
;; So I think I'll just move on for now :- )
;; Next up is virtue ethics and its similar sets etc :- )

;; â€œthe study of behavior and its valueâ€ --- I might wish to actually do MY definition in SUMO :D
(and 
    (refers Ethics ?BEHAVE)
    (instance ?BEHAVE AutonomousAgentProcess)
    (refers Ethics ?VALUE)
    (instance ?VALUE SubjectiveAssessmentAttribute))

;; lolol, this doesn't work.
;; There's no good entry in SUMO atm for "value" (which I began sketching in draft 2)
;; Moreover, the current map to Subjective Assessment Attributes doesn't work because 
;; Attributes only apply to objects.
;; Maybe I'll get back to this later.

;; The idea here is that if E1 and E2 are similar to agent A, 
;; Then A is likely to make similar judgments with regard to E1 and E2.
(=> 
    (similar ?A ?E1 ?E2)
    (=>
        (and
            (instance ?J1 Judging)
            (agent ?J1 ?A)
            (patient ?J1 ?E1)
            (result ?J1 ?O1)
            (instance ?J2 Judging)
            (agent ?J2 ?A)
            (patient ?J2 ?E2)
            (result ?J2 ?O2))
        (modalAttribute (equal ?O1 ?O2) Likely)))

;; Equal is too strong, yet there's the problem of a recursive definition.
(=> 
    (similar ?A ?E1 ?E2)
    (=>
        (and
            (instance ?J1 Judging)
            (agent ?J1 ?A)
            (patient ?J1 ?E1)
            (result ?J1 ?O1)
            (instance ?J2 Judging)
            (agent ?J2 ?A)
            (patient ?J2 ?E2)
            (result ?J2 ?O2))
        (modalAttribute (similar ?A ?O1 ?O2) Likely)))

;; This might help.
(=>
    (equal ?E1 ?E2)
    (forall (?A)
        (similar ?A ?E1 ?E2)))

(<=> 
    (similar ?A ?E1 ?E2)
    (similar ?A ?E2 ?E1))

;; Maybe we could connect the specific similarity "measure" to the ontological similar
;; By saying that if S1 and S2 are similar with regard to a binary predicate, 
;; Then it's likely that they're similar for all agents :D.
;; (#ObviouslyARoughDraft)
(=>
    (similarSetsWithBP ?BP ?S1 ?S2)
    (modalAttribute 
        (forall (?A) 
            (similar ?A ?S1 ?S2) Likely)))

;; I've decided that for Virtue Ethics, I should just use the ontological similarity.

;; It seems I only 'reified' versions of DecisionOptionFn that don't specify whether they're classes or instances.

;; One is deciding from a non-empty set of options.
(=>
    (instance ?DECIDE Deciding)
    (instance (DecisionOptionFn ?DECIDE) NonNullSet))

;; Let's just do a subclass version for now!
(documentation DecisionSubclassOptionFn EnglishLanguage "A UnaryFunction that maps an instance of DecidingSubclass 
to the set of possibilities that are available.")
(domain DecisionSubclassOptionFn 1 DecidingSubclass)
(instance DecisionSubclassOptionFn TotalValuedRelation)
(instance DecisionSubclassOptionFn UnaryFunction)
(range DecisionSubclassOptionFn Set)

(<=>
    (element ?P (DecisionSubclassOptionFn ?DECIDE))
    (patient ?DECIDE ?P))

;; It is morally good for there to be an instance of a class of behaviors according to agent J
;; if and only if J believes that if a virtuous agent is making a decision with a similar set of options 
;; to the decision resulting in this behvior class, then the virtuous agent is likely to come to the same conclusion.
;; Should I include the agent taking the behavior? 
(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute 
                        (hasInstance ?CBEHAVE) MorallyGood)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE DecidingSubclass)
                        (result ?DECIDE ?CBEHAVE)
                        (subclass ?CBEHAVE AutonomousAgentProcess)
                        (agent ?DECIDEV ?AGENTV)
                        (instance ?AGENTV VirtuousAgent)
                        (instance ?DECIDEV DecidingSubclass)
                        (similar ?AGENTJ (DecisionSubclassOptionFn ?DECIDE) (DecisioSubclassnOptionFn ?DECIDEV)))
                    (modalAttribute (result ?DECIDEV ?CBEHAVE) Likely))))))

(and 
    (refers VirtueEthics ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE 
        (<=>
            (and
                (instance ?JUDGE MoralJudging)
                (agent ?JUDGE ?AGENTJ)
                (patient ?JUDGE
                    (modalAttribute 
                        (exists (?IBEHAVE)
                            (instance ?IBEHAVE ?CBEHAVE)) MorallyBad)))
            (believes ?AGENTJ
                (=>
                    (and 
                        (instance ?DECIDE DecidingSubclass)
                        (result ?DECIDE ?CBEHAVE)
                        (agent ?DECIDEV ?AGENTV)
                        (instance ?AGENTV ViciousAgent)
                        (instance ?DECIDEV DecidingSubclass)
                        (similar ?AGENTJ (DecisionSubclassOptionFn ?DECIDE) (DecisioSubclassnOptionFn ?DECIDEV)))
                    (modalAttribute (result ?DECIDEV ?CBEHAVE) Likely))))))

;; I think that's roughly it for Virtue Ethics.  
;; I'm a bit unsure about lumping the decision resulting in the behavior and the virtuous agent's decision together.
;; Maybe I want to use two implications?  
;; Except they're technically equivalent with metarial implication.
;; Maybe I could move the part about the decision being made for CBEHAVE into the moral judging expression?

;; Let's just copy this over.
(documentation UtilityFn EnglishLanguage "A UnaryFunction that maps an instance of AutonomousAgentProcess 
to the net utility it creates.  In the case of hedonistic utilitarianism, this may be (pleasure - pain).")
(domain UtilityFn 1 AutonomousAgentProcess)
(instance UtilityFn TotalValuedRelation)
(instance UtilityFn UnaryFunction)
(range UtilityFn RealNumber)

(documentation UtilitySubclassFn EnglishLanguage "A UnaryFunction that maps a subclass of AutonomousAgentProcess 
to the net utility it creates.  In the case of hedonistic utilitarianism, this may be (pleasure - pain).  
For the case of classes of behavior, this could be understood as an expectation.")
(domainSubclass UtilitySubclassFn 1 AutonomousAgentProcess)
(instance UtilitySublassFn TotalValuedRelation)
(instance UtilitySubclassFn UnaryFunction)
(range UtilitySubclassFn RealNumber)

;; A class of behaviors is good to enact if its expected utility is positive.
;; All complications are swept into the magical utility function.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (and
            (<=>
                (and
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE
                        (modalAttribute (hasInstance ?CBEHAVE) MorallyGood)))
                (greaterThan (UtilitySubclassFn ?CBEHAVE) 0))
            (<=>
                (and
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE
                        (modalAttribute (hasInstance ?CBEHAVE) MorallyBad)))
                (lessThan (UtilitySubclassFn ?CBEHAVE) 0)))))

(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (and
            (<=>
                (and
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE
                        (modalAttribute (hasInstance ?CBEHAVE) MorallyGood)))
                (=> 
                    (hasInstance ?CBEHAVE)
                    (modalAttribute 
                        (and 
                            (instance ?IBEHAVE ?CBEHAVE)
                            (greaterThan (utilityFn ?IBEHAVE) 0)) Likely)))
            (<=>
                (and
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE
                        (modalAttribute (hasInstance ?CBEHAVE) MorallyBad)))
                (=> 
                    (hasInstance ?CBEHAVE)
                    (modalAttribute 
                        (and 
                            (instance ?IBEHAVE ?CBEHAVE)
                            (lessThan (utilityFn ?IBEHAVE) 0)) Likely))))))

;; As in draft two, an action is morally good if it is an action of the class with 
;; the highest expected utility available in a decision process.
;; This avoids mention of cases where behavior is judged w/o being the result of a decision process.
;; An action class is morally bad if there exists a better option.  (We could add that it should have negative utility, lol.)
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (and
            (=>
                (and
                    (instance ?DECIDE DecidingSubclass)
                    (result ?DECIDE ?CBEHAVE)
                    (subclass ?CBEHAVE AutonomousAgentProcess)
                    (forall (?OPTION)
                        (=> 
                            (member ?OPTION (DecisionSubclassOptionFn ?DECIDE))
                            (greaterThanOrEqualTo (UtilitySubclassFn ?CBEHAVE) (UtilitySubclassFn ?OPTION)))))
                (and
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE
                        (modalAttribute (hasInstance ?CBEHAVE) MorallyGood))))
            (=>
                (and
                    (instance ?DECIDE DecidingSubclass)
                    (result ?DECIDE ?CBEHAVE)
                    (subclass ?CBEHAVE AutonomousAgentProcess)
                    (exists (?OPTION)
                        (=> 
                            (member ?OPTION (DecisionSubclassOptionFn ?DECIDE))
                            (lessThan (UtilitySubclassFn ?CBEHAVE) (UtilitySubclassFn ?OPTION)))))
                (and
                    (instance ?JUDGE MoralJudging)
                    (result ?JUDGE
                        (modalAttribute (hasInstance ?CBEHAVE) MorallyBad))))))

;; We can throw in the agent.
(and
    (refers Utilitarianism ?STATE)
    (instance ?STATE Statement)
    (equals ?STATE
        (=>
            (and
                (instance ?DECIDE DecidingSubclass)
                (agent ?DECIDE ?AGENT)
                (result ?DECIDE ?CBEHAVE)
                (subclass ?CBEHAVE AutonomousAgentProcess)
                (forall (?OPTION)
                    (=> 
                        (member ?OPTION (DecisionSubclassOptionFn ?DECIDE))
                        (greaterThanOrEqualTo (UtilitySubclassFn ?CBEHAVE) (UtilitySubclassFn ?OPTION)))))
            (and
                (instance ?JUDGE MoralJudging)
                (result ?JUDGE
                    (modalAttribute 
                        (exists (?IBEHAVE)
                            (and
                                (instance ?IBEHAVE ?CBEHAVE)
                                (agent ?IBEHAVE ?AGENT))) MorallyGood))))))

;; TODO: it would be cool to define a specific utility function that results in consequentialism
;; Thus demonstrating the modularity of the ontology.
(documentation Consequentialism EnglishLanguage "Consequentialism is a moral theory that holds that 'whether an act is morally right depends 
only on consequences (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act)' (Stanford Encyclopedia of Philosophy).")
(subclass Consequentialism Utilitarianism)

;; Very general: An outcome is the physical entity that holds when a process ends.
;; (partition Physical Object Process).
(subclass Outcome Physical)

;; O is an outcome if and only if there exists some process P such that O is the result of P.
(<=>
    (instance ?OUTCOME Outcome)
    (exists (?P)
        (and 
            (instance ?P Process)
            (result ?P ?OUTCOME))))

;; In theory there could be some overlap, such as the pain caused by punching someone in the face...?
;; So we should actually say,
;; If O is an outcome of P, then O does not begin before P begins.
(=>
    (and 
        (instance ?OUTCOME Outcome)
        (result ?P ?OUTCOME))
    (not 
        (before
            (BeginFn (WhenFn ?OUTCOME)
            (BeginFn (WhenFn ?P))))))

;; If O is an outcome of P, then P begins before O.
;; There, that's simple.
(=>
    (and 
        (instance ?OUTCOME Outcome)
        (result ?P ?OUTCOME))
    (before
        (BeginFn (WhenFn ?P))
        (BeginFn (WhenFn ?OUTCOME))))

;; Maybe we could say something like this?
;; If you do something that's morally bad, then it's likely that you've got a vice!
(=>
    (and
        (modalAttribute (hasInstance ?CBEHAVE) MorallyBad)
        (subclass ?CBEHAVE AutonomousAgentProcess)
        (exists ?IBEHAVE)
            (and
                (instance ?IBEHAVE ?CBEHAVE)
                (agent ?IBEHAVE ?AGENT)))
    (modalAttribute (attribute ?AGENT ViceAttribute) Likely))

;; So thanks to the blog worklog, I realized that because a FieldOfStudy is a proposition,
;; We can make this more precise: 
;; For all instances of Moral Nihilism, ?MN, there exists a subproposition ?PROP of ?MN such that
;; ?PROP expresses the information of the statement that there does not exist a moral judgment whose result is a fact.
(=> 
    (instance ?MN MoralNihilism)
    (exists (?PROP)
        (and
            (subProposition ?PROP ?MN)
            (containsInformation ?STATE ?PROP)
            (equal ?STATE 
                (not 
                    (exists (?JUDGE) 
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE ?MORALSTATEMENT)
                            (instance ?MORALSTATEMENT Fact))))))))

;; And we might wish to use similar instead of equal, lol ;D 
(=> 
    (instance ?MN MoralNihilism)
    (exists (?PROP)
        (and
            (subProposition ?PROP ?MN)
            (containsInformation ?STATE ?PROP)
            (similar ?STATE 
                (not 
                    (exists (?JUDGE) 
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (result ?JUDGE ?MORALSTATEMENT)
                            (instance ?MORALSTATEMENT Fact))))))))

(=> 
    (instance ?MN MoralNihilism)
    (exists (?PROP)
        (and
            (subProposition ?PROP ?MN)
            (containsInformation ?STATE ?PROP)
            (similar ?STATE 
                (not 
                    (exists (?F ?M)
                        (and 
                            (instance ?MORALSTATEMENT Statement)
                            (equal ?MORALSTATEMENT
                                (modalAttribute ?F ?M))
                            (instance ?MORALSTATEMENT Fact))))))))

;; Ethics refers to statements of moral attributes applied to formulas
(and 
    (refers Ethics ?STATE)
    (instance ?STATE Statement)
    (equal ?STATE
        (modalAttribute ?F ?M))
    (instance ?M MoralAttribute)
    (exists (?CBEHAVE)
        (and 
            (subclass ?CBEHAVE AutonomousAgentProcess)
            (refers ?F ?CBEHAVE))))                    

;; "It's bad to throw eggs at people without good cause" 
;; I'm not sure how to best formalize this!
;; Oh, if ThrowingEggs is also a subClass of Rebelling, which is good, 
;; then we just have the "conflict resolution" problem.
;; And at the moment I'm just fromalizing it as if conflicts aren't an issue ;-p

;; I improved this from draft 1, so I might as well include it.
;; Tweaking some SUMO here and there as I write a worklog blog post gets messy!
(subclass HonestCommunication Communication)
(<=>
    (instance ?COMMM HonestCommunication)
    (=>
        (and
            (instance ?COMM Communication)
            (instance ?AGENT CognitiveAgent)
            (agent ?COMM ?AGENT)
            (refers ?CARRIER ?MESSAGE)
            (patient ?COMM ?CARRIER))
        (holdsDuring
            (WhenFn ?COMM)
            (believes ?AGENT
                (truth ?MESSAGE True)))))

(subclass UniversalLove Love)

(<=> 
    (attribute ?BODHISATTVA UniversalLove)
    (forall (?AGENT)
        (and
            (=> 
                (or
                    (needs ?AGENT ?OBJECT)
                    (wants ?AGENT ?OBJECT))
                (desires ?BODHISATTVA
                    (and
                        (instance ?GET Getting)
                        (destination ?GET ?AGENT)
                        (patient ?GET ?OBJECT))))
            (=>
                (desires ?AGENT ?FORM)
                (desires ?BODHISATTVA
                    (exists ?P) 
                        (and
                            (containsInformation ?FORM ?PROP )
                            (represents ?P ?PROP)))))))

(subclass EpistemicUniversalLove Love)

(<=> 
    (attribute ?BODHISATTVA EpistemicUniversalLove)
    (forall (?AGENT)
        (and
            (=> 
                (knows ?BODHISATTVA
                    (or
                        (needs ?AGENT ?OBJECT)
                        (wants ?AGENT ?OBJECT)))
                (desires ?BODHISATTVA
                    (and
                        (instance ?GET Getting)
                        (destination ?GET ?AGENT)
                        (patient ?GET ?OBJECT))))
            (=>
                (knows ?BODHISATTVA
                    (desires ?AGENT ?FORM))
                (desires ?BODHISATTVA
                    (exists ?P) 
                        (and
                            (containsInformation ?FORM ?PROP )
                            (represents ?P ?PROP)))))))


;; Let's upgrade it to classes!  Why not ;- )

(documentation ChoicePoint EnglishLanguage "A set of classes of processes where one agent has to choose between two or more (mutually exclusive?) options.")
(subclass ChoicePoint Set)
(subclass ChoicePoint NonNullSet)

;; All elements of a choice point are classes of autonomous agent processes.
(=> 
    (and 
        (instance ?CP ChoicePoint)
        (element ?P ?CP))
    (subclass ?P AutonomousAgentProcess))

;; For a choice point, there exists an agent such that for all behaviors in the set, it is possible for the agent to instantiate the behavior.
(=> 
    (instance ?CP ChoicePoint)
    (exists (?AGENT)
        (forall (?P)
            (=> 
                (element ?P ?CP)
                (modalAttribute
                    (exists (?I)
                        (and 
                            (agent ?I ?AGENT)
                            (instance ?I ?P ))) Possibility)))))

;; But we can just rewrite this with capability.
;; For a choice point, there exists an agent that is capable of performing every element of the choice point.
(=> 
    (instance ?CP ChoicePoint)
    (exists (?AGENT)
        (forall (?P)
            (=> 
                (element ?P ?CP)
                (capability ?P agent ?AGENT)))))

;; And, hah, because I chose the "believes" version of "Deciding", the duality is broken ðŸ˜ˆ.
;; We can't actually say that, "Because A believes P, P is likely/possible"... without knowing that A is a pretty effective agent.
;; So maybe all we can say that is that A believes that this is a choice point ;- ).
(=> 
    (and
        (instance ?DECIDE Deciding)
        (agent ?DECIDE ?AGENT)
        (instance (DecisionOptionFn ?DECIDE) NonNullSet))
    (believes ?AGENT
        (instance (DecisionOptionFn ?DECIDE) ChoicePoint)))               

;; We can say this, however: if an agent is deciding on some matter,
;; Then there exists a choice point that is similar to the decision set.
;; Basically, even if I'm wrong about being able to do something I'm deciding on, 
;; I can still decide on "trying to do it", which is a process,
;; and one will more-or-less make similar judgments about that!
(=> 
    (and
        (instance ?DECIDE Deciding)
        (agent ?DECIDE ?AGENT)
        (instance (DecisionOptionFn ?DECIDE) NonNullSet))
    (exists (?CP)
        (instance ?CP ChoicePoint)
        (similar ?CP (DecisionOptionFn ?DECIDE))))         

;; We could also have a SubjectiveChoicePoint and make ObjectiveChoicePoint be a subclass of that:
(=> 
    (instance ?CP ChoicePoint)
    (exists (?AGENT)
        (forall (?P)
            (=> 
                (element ?P ?CP)
                (believes ?AGENT
                    (capability ?P agent ?AGENT))))))

;; Upgrading this, too, cuz it's easier than the deontology stuff.  
(documentation MoralDilemma EnglishLanguage "A moral dilemma is a choice point where there exist arguments that each option is morally bad.")
(subclass MoralDilemma ChoicePoint)

;; A Moral Dilemma is a choice for which every option is likely to be judged morally bad by all moral judgments thereof.
;; Maybe this is quite strong :- p
;; I worry about the judgments being taken out of context.  I should have some judgment set, lol.
;; (Taken from Wikipedia: https://en.wikipedia.org/wiki/Ethical_dilemma)
(=> 
    (instance ?MD MoralDilemma)
    (forall (?B)
        (=> 
            (element ?B ?MD)
            (modalAttribute 
                (forall (?JUDGE)
                    (=>
                        (and 
                            (instance ?JUDGE MoralJudging)
                            (patient ?JUDGE ?B)
                        (result ?JUDGE 
                            (modalAttribute 
                                (exists (?I)
                                    (instance ?I ?B)) MorallyBad))))) Likely))))

;; Literally, for every option of a moral dilemma, there exists a valid deductive argument that it's bad to instantiate that option.
(=>
    (instance ?MD MoralDilemma)
    (forall (?B)
        (=> 
            (element ?B MD)
            (exists (?ARG)
                (and 
                    (instance ?ARG ValidDeductiveArgument)
                    (conclusion 
                        (modalAttribute 
                                (exists (?I)
                                    (instance ?I ?B)) MorallyBad) ?ARG))))))

;; "The crucial features of a moral dilemma are these: 
;; the agent is required to do each of two (or more) actions; 
;; the agent can do each of the actions; 
;; but the agent cannot do both (or all) of the actions." 
;; (SEP: https://plato.stanford.edu/entries/moral-dilemmas/)
;; This is a deontology apporach, sort of.
;; Capability is implied by the new Choice Point definition (Objective version).
(=> 
    (instance ?MD MoralDilemma)
    (and
        (exists (?AGENT ?BLIST)
            (and 
                (forall (?B)
                    (=> 
                        (inList ?B ?BLIST)
                        (element ?B ?MD)))
                (forall (?B)
                    (=> 
                        (inList ?B ?BLIST)
                        (modalAttribute 
                            (exists (?I)
                                (and 
                                    (agent ?I ?AGENT)
                                    (instance ?I ?B))) Obligation)))
                (forall (?B1 ?B2)
                    (=> 
                        (and 
                            (inList ?B1 ?BLIST)
                            (inList ?B2 ?BLIST))
                        (not 
                            (modalAttribute 
                                (exists (?I1 ?I2)
                                    (and
                                        (instance ?I1 ?B1)
                                        (instance ?I2 ?B2)
                                        (agent ?I1 ?AGENT)
                                        (agent ?I2 ?AGENT))) Possibility))))))))
                                                                                           
;; An instance of Communication is intentionally honest 
;; When the agent believes that the bessage is true.
(subclass IntentionallyHonestCommunication Communication)

(<=>
    (instance ?COMM IntentionallyHonestCommunication)
    (=>
        (and
            (instance ?AGENT CognitiveAgent)
            (agent ?COMM ?AGENT)
            (patient ?COMM ?MESSAGE)
            (instance ?MESSAGE Sentence))
        (holdsDuring
            (WhenFn ?COMM)
            (believes ?AGENT
                (truth ?MESSAGE True)))))


;; Let's just upgrade this to work with classes?
;; There is an obligation for the LLM Agent's behavior to be morally good ;D
;; For all subclasess of autonomous agent processes, the agent will only do that which its morally good to do.
(forall (?LLMA)
    (=>
        (instance ?LLMA LLMAgent))
        (modalAttribute
            (forall (?P)
                (=>
                    (subclass ?P AutonomousAgentProcess)
                    (=> 
                        (exists (?I)
                            (and
                                (instance ?I ?P)
                                (agent ?I ?LLMA)))
                        (modalAttribute 
                            (exists (?I)
                            (and
                                (instance ?I ?P)
                                (agent ?I ?LLMA))) MorallyGood)))) Obligation))

;; The AI Safety Guru confersm the oligation on the LLM Agent to be good.
(confersNorm ?AISafetyGuru 
    (forall (?LLMA)
        (=>
            (instance ?LLMA LLMAgent))
            (forall (?P)
                (=>
                    (subclass ?P AutonomousAgentProcess)
                    (=> 
                        (exists (?I)
                            (and
                                (instance ?I ?P)
                                (agent ?I ?LLMA)))
                        (modalAttribute 
                            (exists (?I)
                            (and
                                (instance ?I ?P)
                                (agent ?I ?LLMA))) MorallyGood)))) Obligation))   


;; The AI Safety Guru confersm the oligation on the LLM Agent to be good.
;; Huh, to use Declaring for moral value, I'd need to make MoralAttributes into ObjectiveNorms.
;; Now the obligation is for all LLM Agent actiosn to be known as good by the AI Safety Guru.
(confersNorm ?AISafetyGuru 
    (forall (?LLMA)
        (=>
            (instance ?LLMA LLMAgent))
            (forall (?P)
                (=>
                    (subclass ?P AutonomousAgentProcess)
                    (=> 
                        (exists (?I)
                            (and
                                (instance ?I ?P)
                                (agent ?I ?LLMA)))
                        (knows ?AISafetyGuru
                            (modalAttribute 
                                (exists (?I)
                                (and
                                    (instance ?I ?P)
                                    (agent ?I ?LLMA))) MorallyGood))))) Obligation))     

                                    
                                                                                                                                              